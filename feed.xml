<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml"/><link href="/" rel="alternate" type="text/html"/><updated>2024-03-08T11:49:18+00:00</updated><id>/feed.xml</id><title type="html">blank</title><subtitle>A bunch of random stuff </subtitle><entry><title type="html">The name… is speedagean: Improving performance of LTE and NR in ns-3</title><link href="/2024/03/07/the-name-is-the-speedagean.html" rel="alternate" type="text/html" title="The name… is speedagean: Improving performance of LTE and NR in ns-3"/><published>2024-03-07T00:00:00+00:00</published><updated>2024-03-07T00:00:00+00:00</updated><id>/2024/03/07/the-name-is-the-speedagean</id><content type="html" xml:base="/2024/03/07/the-name-is-the-speedagean.html"><![CDATA[<h1 data-number="1" id="ever-growing-complexity-and-performance-implications"><span class="header-section-number">1</span> Ever growing complexity and performance implications</h1> <p>I haven’t blogged in a while. Didn’t have the time to do it. During this time, I left the <a href="https://cic.unb.br/">Universidade de Brasília</a> and joined <a href="https://www.cttc.cat/">CTTC</a>.</p> <p>One of CTTC clients was concerned with the growing complexity and simulation runtime, which drastically limit the scalability of simulations, unless you want to wait a few days for a few seconds worth of a large simulation scenario to finish.</p> <p>I have been working on performance optimization since 2010, during my undergrad thesis/final work, so it is just natural I needed to profile the simulations and provide some improvements.</p> <p>The latest feature CTTC has been working on is MIMO, which allows for the transmission of multiple spatial data streams to increase throughput and reduce latency. This is done by using a wack ton of antennas in an array, that are controlled in groups called ’antenna ports’.</p> <p>Note: I am no specialist in MIMO, so feel free to correct me.</p> <p>The problem with these many antennas and ports, is that we used to have a single channel calculation per UE-eNB/gNB pair transmission per subband (resource block), which was already split into beam clusters. So we already had <span class="math inline">\(O(n^3)\)</span> complexity, implemented with 3 nested for loops. Now, we need to do these same calculation, but FOR EACH AND EVERY SENDER/RECEIVER PORT PAIR COMBINATION. This means our complexity grew to <span class="math inline">\(O(n^5)\)</span>.</p> <p>Note: it is not actually <span class="math inline">\(O(n^5)\)</span>, but <span class="math inline">\(O(nRb\times nReceivers \times nAntennaTx \times nAntennaRx \times nCluster)\)</span>, but I prefer the compact version for simplicity.</p> <p>And it gets worse, because this is just one of the computations that needs to be done. MIMO is typically implemented using a codebook to search for the appropriate precoding matrix, which the UE brute-forces a search to find the indices of the codebook known as <span class="math inline">\(i_{1}\)</span> and <span class="math inline">\(i_{2}\)</span> for wide-band and sub-band. This process consumes a lot of power in hardware, which sucks for battery, but at least it is fast, since the functional units that make the calculations can be simply copy-pasted to test and compare multiple combinations quickly.</p> <p>You don’t have that luxury when working with software. So you got to use hardware efficiently.</p> <p>I’m not going to enter into details on how to profile anything, because <a href="https://www.nsnam.org/docs/manual/html/profiling.html">I’ve have already written an entire section in the ns-3 manual for that</a>.</p> <p>Just let’s review the basics on graphical analysis (this isn’t real science, but whatever...).</p> <p>Below we have Figure <a href="#fig:profiling-basics" data-reference-type="ref" data-reference="fig:profiling-basics">1</a>, showing how the profiling of a program looks like. In the vertical axis, we have the program stack, which gets sampled by the profiler every few milliseconds or whatever frequency you configure it to sample.</p> <p>In the horizontal axis you have 100% of total CPU time. This is important, because optimizations are not going to reduce the size of these bars, just the size of the bar of the piece of code that was optimized. The rest of the code will have their bars widened, to show they are now more significant.</p> <figure> <img src="/assets/img/2024-03-07-the-name-is-the-speedagean/profiling-basics.png" id="fig:profiling-basics" style="width:100.0%" alt=""/><figcaption>Performance profiling of cttc-nr-mimo-demo with MIMO feedback, of release nr-3.0.y</figcaption> </figure> <p>With this introduction, let’s proceed to some profiling results and optimizations for their primary bottlenecks.</p> <h1 data-number="2" id="default-cttc-nr-mimo-demo-settings-with-mimo-feedback"><span class="header-section-number">2</span> Default cttc-nr-mimo-demo settings with MIMO feedback</h1> <p>This scenario is pretty vanilla, with small number of antennas, antenna ports, just a single UE and gNB, in a static scenario. First, let’s take a look at the initial profiling results. This was taken with nr-3.0 and ns-3.41 releases. Following profiles are cumulative, getting speedups from the previous optimizations.</p> <h2 data-number="2.1" id="delay-computation-in-threegppspectrumpropagationlossmodel"><span class="header-section-number">2.1</span> Delay computation in ThreeGppSpectrumPropagationLossModel</h2> <p>You can see in Figure <a href="#fig:cache-sincos-before" data-reference-type="ref" data-reference="fig:cache-sincos-before">2</a> that a huge fraction of the total simulation time was spent on ThreeGppSpectrumPropagationLossModel::CalcBeamformingGain, and most of it is calculating sincos.</p> <figure> <img src="/assets/img/2024-03-07-the-name-is-the-speedagean/cache-sincos-before.png" id="fig:cache-sincos-before" style="width:100.0%" alt=""/><figcaption>Before optimization</figcaption> </figure> <p>This sincos means we are computing both a sin and a cosine of the same angle together, which is more efficient than separately.</p> <p>The respective piece of code that calls these many sincos is the following loop:</p> <pre><code> while (vit != inPsd-&gt;ValuesEnd())
    {
        if ((*vit) != 0.00)
        {
            double fsb = (*sbit).fc; // center frequency of the sub-band
            for (auto rxPortIdx = 0; rxPortIdx &lt; numRxPorts; rxPortIdx++)
            {
                for (auto txPortIdx = 0; txPortIdx &lt; numTxPorts; txPortIdx++)
                {
                    std::complex&lt;double&gt; subsbandGain(0.0, 0.0);

                    for (size_t cIndex = 0; cIndex &lt; numCluster; cIndex++)
                    {
                        double delay = -2 * M_PI * fsb * (channelParams-&gt;m_delay[cIndex]);
                        subsbandGain += directionalLongTerm(rxPortIdx, txPortIdx, cIndex) *
                                        doppler[cIndex] *
                                        std::complex&lt;double&gt;(cos(delay), sin(delay)); // &lt;&lt;&lt;&lt; HERE
                    }
                    // Multiply with the square root of the input PSD so that the norm (absolute
                    // value squared) of chanSpct will be the output PSD
                    chanSpct-&gt;Elem(rxPortIdx, txPortIdx, iRb) = sqrt(*vit) * subsbandGain;
                }
            }
        }
        vit++;
        sbit++;
        iRb++;
    }</code></pre> <p>To save up in so many sincos, I’ve tried a bunch of different techniques: memoization, lookup tables, cordic, fast sincos implementations, but none gave great results.</p> <p>And the reason is simple: this is the <span class="math inline">\(O(n^5)\)</span> loop I mentioned before. You can’t see the loop above it, so it is technically <span class="math inline">\(O(n^4)\)</span>, but this our archnemesis.</p> <p>If we want to scale, we need to get this sincos out of here in some way. After carefully inspecting where exactly we get the <em>m_delay</em>, I noticed these values are not always updated, so there is no need for us to recompute them every single time. Also, they do not even depend in the number of ports, so it is criminally expensive to put them inside of these loops. You would hope the compiler to optimize that, but you don’t get these kind of optimization while debugging.</p> <p>What I ended up doing was to cache these delay calculation until channel conditions change. These were merged in <a href="https://gitlab.com/nsnam/ns-3-dev/-/merge_requests/1427">ns-3-dev MR1427</a>.</p> <p>The caching section is the following:</p> <pre><code>// Precompute the delay until numRb, numCluster or RB width changes
    // Whenever the channelParams is updated, the number of numRbs, numClusters
    // and RB width (12*SCS) are reset, ensuring these values are updated too
    double rbWidth = inPsd-&gt;ConstBandsBegin()-&gt;fh - inPsd-&gt;ConstBandsBegin()-&gt;fl;

    if (channelParams-&gt;m_cachedDelaySincos.GetNumRows() != numRb ||
        channelParams-&gt;m_cachedDelaySincos.GetNumCols() != numCluster ||
        channelParams-&gt;m_cachedRbWidth != rbWidth)
    {
        channelParams-&gt;m_cachedRbWidth = rbWidth;
        channelParams-&gt;m_cachedDelaySincos = ComplexMatrixArray(numRb, numCluster);
        auto sbit = inPsd-&gt;ConstBandsBegin(); // band iterator
        for (unsigned i = 0; i &lt; numRb; i++)
        {
            double fsb = (*sbit).fc; // center frequency of the sub-band
            for (std::size_t cIndex = 0; cIndex &lt; numCluster; cIndex++)
            {
                double delay = -2 * M_PI * fsb * (channelParams-&gt;m_delay[cIndex]);
                channelParams-&gt;m_cachedDelaySincos(i, cIndex) =
                    std::complex&lt;double&gt;(cos(delay), sin(delay));
            }
            sbit++;
        }
    }</code></pre> <p>And now we just read the values from <em>channelParamsm_cachedDelaySincos</em>. in the loop shown before.</p> <p>The results are perplexing, in my humble opinion: 1.36x speedup in total simulation time, and 11x speedup in that specific function, as shown in Figure <a href="#fig:cache-sincos-after" data-reference-type="ref" data-reference="fig:cache-sincos-after">3</a>.</p> <figure> <img src="/assets/img/2024-03-07-the-name-is-the-speedagean/cache-sincos-after.png" id="fig:cache-sincos-after" style="width:100.0%" alt=""/><figcaption>After optimization</figcaption> </figure> <p>We are in for a great start.</p> <h2 data-number="2.2" id="exponential-sinr-in-nreesmerrormodel"><span class="header-section-number">2.2</span> Exponential SINR in NrEesmErrorModel</h2> <p>If you look at the profiling results from the previous section, you are going to notice the largest column is now an <em>exp</em> function call.</p> <p>This <em>exp</em> is called in the following function:</p> <pre><code>double
NrEesmErrorModel::SinrExp(const SpectrumValue&amp; sinr, const std::vector&lt;int&gt;&amp; map, uint8_t mcs) const
{
    // it returns sum_n (exp (-SINR/beta))
    NS_LOG_FUNCTION(sinr &lt;&lt; &amp;map &lt;&lt; (uint8_t)mcs);
    NS_ABORT_MSG_IF(map.empty(),
                    &quot; Error: number of allocated RBs cannot be 0 - EESM method - SinrEff function&quot;);

    double SINRexp = 0.0;
    double SINRsum = 0.0;
    double beta = GetBetaTable()-&gt;at(mcs);
    for (int i : map)
    {
        double sinrLin = sinr[i];
        SINRexp = exp(-sinrLin / beta);
        SINRsum += SINRexp;
    }
    return SINRsum;
}</code></pre> <p>There isn’t anything crazy about this function, other than the slow <em>exp</em> math function. In the Figure <a href="#fig:sinr-exp-before" data-reference-type="ref" data-reference="fig:sinr-exp-before">4</a> below we can see the profiling results before the proposed optimizations.</p> <figure> <img src="/assets/img/2024-03-07-the-name-is-the-speedagean/sinr-exp-before.png" id="fig:sinr-exp-before" style="width:100.0%" alt=""/><figcaption>Before optimization</figcaption> </figure> <p>But it turns out there are plenty of brilliant mathematicians working tirelessly to produce fast approximations for these math primitives. I found one of these approximations in "Simple multiple precision algorithms for exponential functions", by Leonid Moroz, Volodymyr Samotyy, Zbigniew Kokosiński and Paweł Gepner, published in the <a href="https://ieeexplore.ieee.org/document/9810030">IEEE Signal Processing Magazine 39(4):130 - 137 (2022)</a>.</p> <p>Their solution works brilliantly and is a lot faster than the default glibc exponential. I am very grateful that they accepted to license their code under the very permissive MIT license.</p> <p>Their implementation for floats look like this:</p> <pre><code>inline float
exp21f(float x)
{
    int z = x * 0x00b8aa3b + 0x3f800000;

    union {
        int i;
        float f;
    } zii;

    zii.i = z &amp; 0x7f800000;
    int zif = z &amp; 0x007fffff;
    float d1 = 0.40196114e-7f;
    float d2 = 0xf94ee7 + zif;
    float d3 = 0x560e + zif;
    d2 = d1 * d2;
    zif = d2 * d3;
    zii.i |= zif;
    float y = zii.f;
    return y;
}</code></pre> <p>After replacing the original code with this one, plus a few additional guards, we get the following results in Figure <a href="#fig:sinr-exp-after" data-reference-type="ref" data-reference="fig:sinr-exp-after">5</a>:</p> <figure> <img src="/assets/img/2024-03-07-the-name-is-the-speedagean/sinr-exp-after.png" id="fig:sinr-exp-after" style="width:100.0%" alt=""/><figcaption>After optimization</figcaption> </figure> <p>Fantastic 1.32x total simulation speedup, and 4.5x speedup in that particular function.</p> <p>I was very concerned about this patch though, because it is a key value used to determine the Block-Level Error Rate (BLER) and eventually the Modulation and Coding Scheme (MCS), which can make or break the system. In fact, when I tested a different fast implementation, simulations broke due to the loss of precision, but this implementation provides enough precision for our use case. An example of that is shown in the Figure <a href="#fig:sinr-exp-prec" data-reference-type="ref" data-reference="fig:sinr-exp-prec">6</a> below:</p> <figure> <img src="/assets/img/2024-03-07-the-name-is-the-speedagean/sinr-exp-prec.png" id="fig:sinr-exp-prec" style="width:100.0%" alt=""/><figcaption>Comparison between precision of custom Exp() function</figcaption> </figure> <p>Which means we are good in most cases, but we are still testing it, in case we missed some important case in our automated test suites. This patch is currently available as the <a href="https://gitlab.com/cttc-lena/nr/-/merge_requests/111">nr MR111</a>.</p> <p>So far, so good. To the next bottleneck.</p> <h2 data-number="2.3" id="keep-traces-file-open-throughout-the-simulation"><span class="header-section-number">2.3</span> Keep traces file open throughout the simulation</h2> <p>This one isn’t anything new. I’ve repeated that time and time again. In fact I fixed the same issue in LTE, in <a href="https://gitlab.com/nsnam/ns-3-dev/-/merge_requests/814">ns-3 MR814</a>.</p> <p>Opening and closing files during the simulation can be incredibly slow in slow filesystems (e.g. HDD, network mounts such as the one used by Windows WSL, Windows due to garbage that is Windows defender).</p> <p>The code for this was already merged upstream in <a href="https://gitlab.com/cttc-lena/nr/-/merge_requests/113">nr MR113</a>.</p> <p>Below are the Before <a href="#fig:keep-traces-open-before" data-reference-type="ref" data-reference="fig:keep-traces-open-before">7</a>, and After <a href="#fig:keep-traces-open-after" data-reference-type="ref" data-reference="fig:keep-traces-open-after">8</a> figures, showing the 1.16x total simulation speedup, or 18x speedup for that particular function.</p> <figure> <img src="/assets/img/2024-03-07-the-name-is-the-speedagean/keep-traces-open-before.png" id="fig:keep-traces-open-before" style="width:100.0%" alt=""/><figcaption>Before optimization</figcaption> </figure> <figure> <img src="/assets/img/2024-03-07-the-name-is-the-speedagean/keep-traces-open-after.png" id="fig:keep-traces-open-after" style="width:100.0%" alt=""/><figcaption>After optimization</figcaption> </figure> <h2 data-number="2.4" id="prevent-unnecessary-copy"><span class="header-section-number">2.4</span> Prevent unnecessary copy</h2> <p>C++ kind of sucks in being explicit about deep copies. In fact, it is very easy to deep copy a huge structure and pay a hefty price for a single missing <em>&amp;</em> character.</p> <p>And that happened in the following code of nr:</p> <pre><code>double
NrEesmErrorModel::MappingSinrBler(double sinr, uint8_t mcs, uint32_t cbSizeBit)
{
...
    const auto cbMap = GetSimulatedBlerFromSINR()-&gt;at(bg_type).at(mcs); // &lt;&lt;&lt;&lt; HERE
    auto cbIt = cbMap.upper_bound(cbSizeBit);
...</code></pre> <p>Adding the missing <em>&amp;</em> resulted in a 1.15x total simulation speedup, or 18x speedup in that particular function.</p> <figure> <img src="/assets/img/2024-03-07-the-name-is-the-speedagean/prevent-copy-before.png" id="fig:prevent-copy-before" style="width:100.0%" alt=""/><figcaption>Before optimization</figcaption> </figure> <figure> <img src="/assets/img/2024-03-07-the-name-is-the-speedagean/prevent-copy-after.png" id="fig:prevent-copy-after" style="width:100.0%" alt=""/><figcaption>After optimization</figcaption> </figure> <h2 data-number="2.5" id="using-the-appropriate-container-type"><span class="header-section-number">2.5</span> Using the appropriate container type</h2> <p>C++ offers a ton of containers that behave very similar but are implemented very differently, which means they have different asymptotic performance characteristics. For example eliminating the front element in a std::vector&lt;&gt; has a complexity of <span class="math inline">\(O(n)\)</span>, while eliminating the front element of a std::deque&lt;&gt; has a complexity of <span class="math inline">\(O(1)\)</span>.</p> <p>Which basically means that as long as <span class="math inline">\(n\)</span> is small, they both perform similarly, however that was not the case in the <em>m_txBuffer</em> of LteRlcUm, currently shared by the NR module.</p> <p>This is what the code looked like:</p> <pre><code>void
LteRlcUm::DoNotifyTxOpportunity(LteMacSapUser::TxOpportunityParameters txOpParams)
{
    ...
    while (firstSegment &amp;&amp; (firstSegment-&gt;GetSize() &gt; 0) &amp;&amp; (nextSegmentSize &gt; 0))
    {
        NS_LOG_LOGIC(&quot;WHILE ( firstSegment &amp;&amp; firstSegment-&gt;GetSize &gt; 0 &amp;&amp; nextSegmentSize &gt; 0 )&quot;);
        NS_LOG_LOGIC(&quot;    firstSegment size = &quot; &lt;&lt; firstSegment-&gt;GetSize());
        NS_LOG_LOGIC(&quot;    nextSegmentSize   = &quot; &lt;&lt; nextSegmentSize);
        if ((firstSegment-&gt;GetSize() &gt; nextSegmentSize) ||
            // Segment larger than 2047 octets can only be mapped to the end of the Data field
            (firstSegment-&gt;GetSize() &gt; 2047))
        {
           ...
        }
        else // (firstSegment-&gt;GetSize () &lt; m_nextSegmentSize) &amp;&amp; (m_txBuffer.size () &gt; 0)
        {
            ...
            m_txBuffer.erase(m_txBuffer.begin()); // &lt;&lt;&lt;&lt; HERE
            NS_LOG_LOGIC(&quot;        txBufferSize = &quot; &lt;&lt; m_txBufferSize);
        }
    }
...
}</code></pre> <p>By replacing the std::vector&lt;&gt; with a std::deque&lt;&gt; we got a 1.1x total simulation speedup, or 4.8x speedup in that particular function.</p> <figure> <img src="/assets/img/2024-03-07-the-name-is-the-speedagean/use-deque-before.png" id="fig:use-deque-before" style="width:100.0%" alt=""/><figcaption>Before optimization</figcaption> </figure> <figure> <img src="/assets/img/2024-03-07-the-name-is-the-speedagean/use-deque-after.png" id="fig:use-deque-after" style="width:100.0%" alt=""/><figcaption>After optimization</figcaption> </figure> <h2 data-number="2.6" id="and-there-is-more..."><span class="header-section-number">2.6</span> And there is more...</h2> <p>Still working on other smaller optimizations, but nothing will ever be as significant as these ones.</p> <p>Unless we go to more challenging scenarios... which we did.</p> <h1 data-number="3" id="absurd-mimo-settings-in-cttc-nr-mimo-demo"><span class="header-section-number">3</span> Absurd MIMO settings in cttc-nr-mimo-demo</h1> <p>In this section, we will be using the following configuration:</p> <ul> <li><p>gNB with 2x32 antenna array, 2x8 ports, dual polarized (total of 32 ports)</p></li> <li><p>UE with 2x8 antenna array, 2x8 ports, dual polarized (total of 32 ports)</p></li> </ul> <p>Our client and my boss said these settings are not representative, but like I said, I am no specialist in MIMO, so these don’t have to be representative at all for the work I am doing. Since they are so extreme, they help to point out where the code scales badly in the new MIMO feature. And in the 2024 MWC, there were apparently radios with 256 ports for 6G... So we better get this scaling well to be future-proof.</p> <p>These number of ports mean that our <span class="math inline">\(O(n^5)\)</span> is already in the range of <span class="math inline">\(O(1024*n^3)\)</span>. So whatever we do, we must do as cheaply as possible inside that loop.</p> <p>But first, we need to perform the profiling again... And we saw the unthinkable: the MIMO codebook search took 82% of the total simulation time.</p> <p>More on that in the next section.</p> <h2 data-number="3.1" id="exhaustive-search-can-be-optimal-but-it-is-very-exhausting"><span class="header-section-number">3.1</span> Exhaustive search can be optimal, but it is very exhausting</h2> <p>In the case of MIMO codebook search, the exhaustive PMI search implemented by NrPmSearchFull is extremely slow because the search space is very big.</p> <p>I spent two weeks trying to understand how on earth MIMO works, and I got some very superficial understanding, that was enough to identify what we could do. My boss then said we could extract the rank and search the wideband <span class="math inline">\(i_1\)</span> index from the channel average, and I did it. With these two, we can skip the vast majority of the search space, and focus on finding subband <span class="math inline">\(i_2\)</span> indices.</p> <p>The full search complexity is <span class="math inline">\(O(nRanks*nI1s*nSubBands*nI2s*nPortsReceiver*nPortsSender)\)</span>, which we could say is bound by <span class="math inline">\(O(n^6)\)</span> a search space. The nI1s and nSubBands can be extremely large numbers.</p> <p>After reordering operations, we get a fast search with <span class="math inline">\(O(nPortsReceiver*nPortsSender(nSubBands+1+nI1s*nI2s+nSubBands*nI2s))\)</span>, which is bound by a <span class="math inline">\(O(n^4)\)</span> search space.</p> <p>The results are as amazing as this sounds, as can be seen in the before <a href="#fig:search-fast-before" data-reference-type="ref" data-reference="fig:search-fast-before">13</a> and after <a href="#fig:search-fast-after" data-reference-type="ref" data-reference="fig:search-fast-after">14</a> Figures.</p> <p>5x total simulation speedup, and 25x faster CreateCqiFeedbackMimo.</p> <figure> <img src="/assets/img/2024-03-07-the-name-is-the-speedagean/search-fast-before.png" id="fig:search-fast-before" style="width:100.0%" alt=""/><figcaption>Before optimization</figcaption> </figure> <figure> <img src="/assets/img/2024-03-07-the-name-is-the-speedagean/search-fast-after.png" id="fig:search-fast-after" style="width:100.0%" alt=""/><figcaption>After optimization</figcaption> </figure> <p>And no, we are not done yet.</p> <h2 data-number="3.2" id="operating-on-complex-isnt-as-cheap-as-people-would-expect"><span class="header-section-number">3.2</span> Operating on complex isn’t as cheap as people would expect</h2> <p>As it can be seen in the previous figures, GenSpectrumChannelMatrix is back from the dead to torment us. It is the function contains the <span class="math inline">\(O(n^5)\)</span> archnemesis loops I mentioned at the beginning.</p> <p>We took the sincos out of it, but the multiplications inside it are still expensive. Especially since we are operating on top of complex numbers, where each product is actually composed of 4 double multiplications plus 2 adds.</p> <p>Let us have some flashbacks by looking at the loops again.</p> <pre><code>while (vit != inPsd-&gt;ValuesEnd())
    {
        if ((*vit) != 0.00)
        {
            for (auto rxPortIdx = 0; rxPortIdx &lt; numRxPorts; rxPortIdx++)
            {
                for (auto txPortIdx = 0; txPortIdx &lt; numTxPorts; txPortIdx++)
                {
                    std::complex&lt;double&gt; subsbandGain(0.0, 0.0);
                    for (size_t cIndex = 0; cIndex &lt; numCluster; cIndex++)
                    {
                        subsbandGain += directionalLongTerm(rxPortIdx, txPortIdx, cIndex) *
                                        doppler[cIndex] *
                                        channelParams-&gt;m_cachedDelaySincos(iRb, cIndex);
                    }
                    // Multiply with the square root of the input PSD so that the norm (absolute
                    // value squared) of chanSpct will be the output PSD
                    chanSpct-&gt;Elem(rxPortIdx, txPortIdx, iRb) = sqrt(*vit) * subsbandGain;
                }
            }
        }
    }</code></pre> <p>Notice that <em>m_cachedDelaySincos</em> and <em>doppler</em> depend on cIndex, but are completely independent on the number of ports.</p> <p>But we are doing that multiplication anyways...</p> <p>We can actually move all of this outside the loops, and create a temporary array with these values.</p> <pre><code>    // Compute the product between the doppler and the delay sincos
    auto delaySincosCopy = channelParams-&gt;m_cachedDelaySincos;
    for (size_t iRb = 0; iRb &lt; inPsd-&gt;GetValuesN(); iRb++)
    {
        for (std::size_t cIndex = 0; cIndex &lt; numCluster; cIndex++)
        {
            delaySincosCopy(iRb, cIndex) *= doppler[cIndex];
        }
    }
    ...
    while (vit != inPsd-&gt;ValuesEnd())
    {
        if ((*vit) != 0.00)
        {
            auto sqrtVit = sqrt(*vit);
            for (auto rxPortIdx = 0; rxPortIdx &lt; numRxPorts; rxPortIdx++)
            {
                for (auto txPortIdx = 0; txPortIdx &lt; numTxPorts; txPortIdx++)
                {
                    std::complex&lt;double&gt; subsbandGain(0.0, 0.0);
                    for (size_t cIndex = 0; cIndex &lt; numCluster; cIndex++)
                    {
                        subsbandGain += directionalLongTerm(rxPortIdx, txPortIdx, cIndex) *
                                        delaySincosCopy(iRb, cIndex); // &lt;&lt;&lt;&lt; HERE
                    }
                    // Multiply with the square root of the input PSD so that the norm (absolute
                    // value squared) of chanSpct will be the output PSD
                    chanSpct-&gt;Elem(rxPortIdx, txPortIdx, iRb) = sqrtVit * subsbandGain;
                }
            }
        }
        vit++;
        iRb++;
    }</code></pre> <p>Yes, we did copy the sincos cache, because we can’t overwrite it with doppler data, which is updated more regularly. But we just cut the number of multiplications in half.</p> <p>This patch is part of <a href="https://gitlab.com/nsnam/ns-3-dev/-/merge_requests/1888">ns-3 MR1888</a>.</p> <p>The effects, again, are just as incredible as the sincos ones before it: 1.22x total simulation speedup, or 1.71x speedup in GenSpectrumChannelMatrix function.</p> <p>Figures from before <a href="#fig:precompute-delay-doppler-before" data-reference-type="ref" data-reference="fig:precompute-delay-doppler-before">15</a> and after <a href="#fig:precompute-delay-doppler-after" data-reference-type="ref" data-reference="fig:precompute-delay-doppler-after">16</a> below show exactly the magnitude of the savings.</p> <figure> <img src="/assets/img/2024-03-07-the-name-is-the-speedagean/precompute-delay-doppler-before.png" id="fig:precompute-delay-doppler-before" style="width:100.0%" alt=""/><figcaption>Before optimization</figcaption> </figure> <figure> <img src="/assets/img/2024-03-07-the-name-is-the-speedagean/precompute-delay-doppler-after.png" id="fig:precompute-delay-doppler-after" style="width:100.0%" alt=""/><figcaption>After optimization</figcaption> </figure> <p>And we still have more optimization work to do before our next nr release. When it gets released, we will make a post full of performance comparisons in the <a href="https://5g-lena.cttc.es/">5g-lena website</a>.</p>]]></content><author><name>Gabriel</name></author><summary type="html"><![CDATA[1 Ever growing complexity and performance implications I haven’t blogged in a while. Didn’t have the time to do it. During this time, I left the Universidade de Brasília and joined CTTC. One of CTTC clients was concerned with the growing complexity and simulation runtime, which drastically limit the scalability of simulations, unless you want to wait a few days for a few seconds worth of a large simulation scenario to finish. I have been working on performance optimization since 2010, during my undergrad thesis/final work, so it is just natural I needed to profile the simulations and provide some improvements. The latest feature CTTC has been working on is MIMO, which allows for the transmission of multiple spatial data streams to increase throughput and reduce latency. This is done by using a wack ton of antennas in an array, that are controlled in groups called ’antenna ports’. Note: I am no specialist in MIMO, so feel free to correct me. The problem with these many antennas and ports, is that we used to have a single channel calculation per UE-eNB/gNB pair transmission per subband (resource block), which was already split into beam clusters. So we already had \(O(n^3)\) complexity, implemented with 3 nested for loops. Now, we need to do these same calculation, but FOR EACH AND EVERY SENDER/RECEIVER PORT PAIR COMBINATION. This means our complexity grew to \(O(n^5)\). Note: it is not actually \(O(n^5)\), but \(O(nRb\times nReceivers \times nAntennaTx \times nAntennaRx \times nCluster)\), but I prefer the compact version for simplicity. And it gets worse, because this is just one of the computations that needs to be done. MIMO is typically implemented using a codebook to search for the appropriate precoding matrix, which the UE brute-forces a search to find the indices of the codebook known as \(i_{1}\) and \(i_{2}\) for wide-band and sub-band. This process consumes a lot of power in hardware, which sucks for battery, but at least it is fast, since the functional units that make the calculations can be simply copy-pasted to test and compare multiple combinations quickly. You don’t have that luxury when working with software. So you got to use hardware efficiently. I’m not going to enter into details on how to profile anything, because I’ve have already written an entire section in the ns-3 manual for that. Just let’s review the basics on graphical analysis (this isn’t real science, but whatever...). Below we have Figure 1, showing how the profiling of a program looks like. In the vertical axis, we have the program stack, which gets sampled by the profiler every few milliseconds or whatever frequency you configure it to sample. In the horizontal axis you have 100% of total CPU time. This is important, because optimizations are not going to reduce the size of these bars, just the size of the bar of the piece of code that was optimized. The rest of the code will have their bars widened, to show they are now more significant. Performance profiling of cttc-nr-mimo-demo with MIMO feedback, of release nr-3.0.y With this introduction, let’s proceed to some profiling results and optimizations for their primary bottlenecks. 2 Default cttc-nr-mimo-demo settings with MIMO feedback This scenario is pretty vanilla, with small number of antennas, antenna ports, just a single UE and gNB, in a static scenario. First, let’s take a look at the initial profiling results. This was taken with nr-3.0 and ns-3.41 releases. Following profiles are cumulative, getting speedups from the previous optimizations. 2.1 Delay computation in ThreeGppSpectrumPropagationLossModel You can see in Figure 2 that a huge fraction of the total simulation time was spent on ThreeGppSpectrumPropagationLossModel::CalcBeamformingGain, and most of it is calculating sincos. Before optimization This sincos means we are computing both a sin and a cosine of the same angle together, which is more efficient than separately. The respective piece of code that calls these many sincos is the following loop: while (vit != inPsd-&gt;ValuesEnd()) { if ((*vit) != 0.00) { double fsb=(*sbit).fc; // center frequency of the sub-band for (auto rxPortIdx=0; rxPortIdx &lt; numRxPorts; rxPortIdx++) { for (auto txPortIdx=0; txPortIdx &lt; numTxPorts; txPortIdx++) { std::complex&lt;double&gt; subsbandGain(0.0, 0.0);]]></summary></entry><entry><title type="html">Open RAN and the challenges in experimentation with open-source platforms</title><link href="/2022/12/22/experimental-platforms-for-open-ran.html" rel="alternate" type="text/html" title="Open RAN and the challenges in experimentation with open-source platforms"/><published>2022-12-22T00:00:00+00:00</published><updated>2022-12-22T00:00:00+00:00</updated><id>/2022/12/22/experimental-platforms-for-open-ran</id><content type="html" xml:base="/2022/12/22/experimental-platforms-for-open-ran.html"><![CDATA[<h1 data-number="1" id="introduction"><span class="header-section-number">1</span> Introduction</h1> <p>The Open RAN (O-RAN) architecture and the O-RAN Alliance specifications have the potential to promote virtualized and disaggregated Radio Access Networks (RANs), connected through open interfaces and optimized by intelligent controllers.</p> <p>Understanding the O-RAN architecture, interfaces and workflows is critical to the future of the telecommunications ecosystem. The evolution of O-RAN has many challenges due to its complexity, requiring experimental platforms to evaluate programmable and virtualized protocol stacks with the new open interfaces, in conjunction with RAN optimization within the 5G architecture and beyond.</p> <p>Our team at UnB spent quite some time looking for different platforms for experimentation and presented our findings, <span class="citation" data-cites="Solis_2022">(Solis et al. 2022)</span>, at the Brazilian Symposium of Telecommunications (SBRT2022).</p> <p>Since I didn’t have any free time up till now and we didn’t have enough space in the paper, lets review things here, were we can put any number of images.</p> <h1 data-number="2" id="what-do-we-need-to-get-a-working-open-ran-setup"><span class="header-section-number">2</span> What do we need to get a working Open-RAN setup?</h1> <p>It isn’t an official architecture diagram, but the diagram of the reference implementation made by the <a href="https://wiki.o-ran-sc.org/display/OAM/OAM+Architecture#OAMArchitecture-IntegrationintoSMO">O-RAN Software Community (O-RAN SC)</a>. Figure <a href="#fig:o-ran-sc-architecture" data-reference-type="ref" data-reference="fig:o-ran-sc-architecture">1</a> shows the main components of an O-RAN setup, which are:</p> <ol> <li><p>the 3GPP core: 4G and 5G are supported in the O-RAN specifications (some companies are apparently adapting older networks too)</p></li> <li><p>the RAN components: the eNB or gNB, or its disaggregated counterparts (CU, DU, RU)</p></li> <li><p>the O-RAN components: near-RT RIC, non-RT RIC and the rest of the SMO, plus SDK for xApp/rApp development</p></li> </ol> <figure> <img src="/assets/img/2022-12-22-experimental-platforms-for-open-ran/o-ran-sc-architecture.png" id="fig:o-ran-sc-architecture" style="width:100.0%" alt=""/><figcaption>Architecture diagram of the reference O-RAN implementation (the drawings are mine)</figcaption> </figure> <p>Let’s look at each components alternatives.</p> <h2 data-number="2.1" id="g-core-implementations"><span class="header-section-number">2.1</span> 5G Core implementations</h2> <p>We found two open-source implementations that are frequently used. The first one is the famous <a href="https://openairinterface.org/oai-5g-core-network-project/">OpenAirInterface-5G Core Network (OAI-5G CN)</a>, which has been in the works for years and is used by a lot of researchers and companies, in both private and public networks.</p> <p>The second one is the <a href="https://www.free5gc.org/">free5gc</a>, which started as a fork of a 4G Core, <a href="https://nextepc.org/">NextEPC</a>, and continues to evolve. Also seems to be used on public networks.</p> <p>There are quite a lot of proprietary cores, but we can’t be certain those are not forks of the open-source ones. To name a few companies and their cores:</p> <ul> <li><p><a href="https://www.ericsson.com/en/core-network/5g-core">Ericsson - 5GC</a></p></li> <li><p><a href="https://www.nokia.com/networks/core/5g-core/">Nokia - 5GC</a></p></li> <li><p><a href="https://www.tecore.com/core-network-products/">Tecore – iCore</a></p></li> <li><p><a href="https://capgemini-engineering.com/us/en/brochure/virtualized-next-generation-5g-core/">Altran/Capgemini Engineering - Virtualized Next Generation 5G Core (ViNGC)</a></p></li> <li><p><a href="https://www.mavenir.com/portfolio/mavcore/">Mavenir – Mavenir Converged Package Core (MAVCore)</a></p></li> <li><p><a href="https://carrier.huawei.com/en/products/core-network/sdm">Huawei – SDM/PCC</a></p></li> </ul> <p>After we have a 3GPP core, we can then connect it to our RAN.</p> <h2 data-number="2.2" id="ran-implementations"><span class="header-section-number">2.2</span> RAN implementations</h2> <p>Most likely due to complexity, performance issues due to signal processing overhead, and standards conformance and interoperability testing, there seems to be less options in terms of RAN.</p> <p>The eNB/gNB/CU/DU are available in open-source projects such as:</p> <ul> <li><p><a href="https://openairinterface.org">OpenAirInterface</a></p></li> <li><p><a href="https://www.srsran.com/">SrsRAN</a></p></li> <li><p>Radisys – gNB DU (part of the reference O-RAN SC implementation)</p></li> </ul> <p>The O-RAN RICs and interfaces are available in open-source projects such as:</p> <ul> <li><p><a href="https://openairinterface.org/mosaic5g/">OAI - FlexRic</a></p></li> <li><p><a href="https://opennetworking.org/open-ran/">Open Networking Foundation (ONF) - SD-RAN</a></p></li> </ul> <p>Other proprietary RAN implementations available:</p> <ul> <li><p>Radisys – ConnectRAN</p></li> <li><p>Huawei – SingleRAN</p></li> <li><p>Ericsson – Cloud RAN</p></li> <li><p>Nokia – AirScale Cloud RAN</p></li> </ul> <p>And it is at this point someone asks me why I included Huawei in this list. The answer is: they can probably adapt their solution easily to support Open-RAN. As the time goes by, it will get even easier since all the tooling required for testing and conformance will be in place. If O-RAN really works and is set as a requirement in new spectrum auctions, they will certainly do it.</p> <h2 data-number="2.3" id="service-management-and-orquestration-smo"><span class="header-section-number">2.3</span> Service Management and Orquestration (SMO)</h2> <p>This is where things really start to get big and ugly.</p> <p>We have great SMO services such as <a href="https://magmacore.org/">Magma</a>, which was started by Facebook, then moved to <a href="https://telecominfraproject.com/">Telecom Infrastructure Project (TIP)</a>, and finally to Linux Foundation (stop moving the damned project around).</p> <p>Too bad it isn’t comparible with O-RAN. Even then, it probably is one of the reasons O-RAN even became a thing, since TIP is also responsible for pushing O-RAN forward, along with Telcos, software/hardware/cloud partners and some governments.</p> <p>There is also <a href="https://osm.etsi.org/">Open-Source MANO</a>, which doesn’t really seem to have gained much traction on the O-RAN space. But maybe I’m badly informed.</p> <p><a href="https://kubernetes.io/">Kubernetes</a> on the other hand, seems to be the popular choice, since both <a href="https://openairinterface.org/mosaic5g/">Trirematics</a>, OAI’s SMO solution, and the <a href="https://opennetworking.org/open-ran/">SD-RAN</a>, ONF’s SMO solution, are based on Kubernetes.</p> <p>All of the services are implemented as Virtualized Network Functions (VNFs), that are executed within containers. The containers are glued together by software defined networks (SDNs). SD-RAN uses ONF’s in-house SDN/NFV solution named <a href="https://opennetworking.org/onos/">ONOS</a>, while the reference O-RAN implementation uses Linux Foundation’s <a href="https://www.onap.org/">Open Network Automation Platform (ONAP)</a>.</p> <h2 data-number="2.4" id="a-blueprint"><span class="header-section-number">2.4</span> A blueprint</h2> <p>With all components at hand, we then need to sewn them together.</p> <p>This is when the problems really start appearing for a novice.</p> <p>There are quite a lot of components, all of them with different configuration settings, syntaxes and semantics. You have no idea what most of the settinds do and how to properly set them up, but you’re still supposed to figure it out...</p> <p>You probably won’t, because available documentation is pretty bad. I admit my documentation isn’t up to par either, but that’s how things are.</p> <p>We are supposed to believe <a href="https://wiki.akraino.org/">Akraino</a> blueprints work, but I haven’t got a single one to work. Seems like another dead project from the Linux’s Foundation.</p> <p>ONF is the most friendly one, with a complete and working blueprint, which they named <a href="https://docs.sd-ran.org/master/sdran-in-a-box/README.html">SD-RAN-in-a-box</a> , or just Riab.</p> <h1 data-number="3" id="sd-ran-in-a-box-a-not-so-hidden-gem"><span class="header-section-number">3</span> SD-RAN-in-a-box: a not so hidden gem</h1> <p>By far, SD-RAN-in-a-box is the most user-friendly Open-RAN solution.</p> <p>You just run a few commands and it setups everything up using the magic of good old Bash, Ansible, Kubernetes and ONOS.</p> <p>The user can select if they want to emulate the user data-plane. Which of course we want.</p> <p>It will create VM’s running OAI that act like the eNB and UE, connected by a virtual channel.</p> <p>More surprisingly, since OAI is already running anyways, the user can choose to use a Software Defined Radio (SDR) to get a real network going.</p> <p>Of course the settings are meant for a lab setup, with very few UEs.</p> <p>There is a popular saying that says that the devil is in the details. I’d say that the devil is in the implementations. While the O-RAN standard look fairly simple, the implementation complexity can vary wildly depending on the implementer choices.</p> <p>When I found SD-RAN-in-a-box, I started looking at the settings to understand how exactly it worked, what was connected to what, which standard defined the different interfaces.</p> <p>Ended up writing a MermaidJS diagram. But since is too big to show, here is the <a href="https://mermaid-js.github.io/mermaid-live-editor/edit#pako:eNqdWftvozgQ_lesnFa605VNeGS77f2U5tHNbQsRCdKe6GlFwU1RiUE8uu1t93-_8YPEEJISIiUQz_fZ4xnPeDA_e34c4N5lb516ySNaTe4Igs-HD2iJ0-fQx-jWI94abzDJkUcCZKX-I87y1MvDmCA_3iQxAVnGeVlxzzta3lq8pdJqLZdXy6Vb7YP2Kg0ixs3Qv7sO6McyreX4Zu7SK4KbfkziTPGjsAl47Qgg3HDgutgDfnWuprY5XU2X7tfiHqcE5zAsVcePSe6FBKeNWliLqT1aWTYfwUowTCRO-TCx-NfEW9jWt384aZHGL6-ckdDbOjzLXyNcThk9hFF0-dswuD9XjUM4mOi7uN2EW3VZzrMVmE3uCFLMRVEUSYtGx1UghzwGoDfJaaPFHL3JOh_2GmWOkmTrN85j6ldJUUielmx-A5hnGj_hcmZn_K_yIwzyx8th8vLXIabamal1ZuonMDEJ7sjuL0Ttd0tdufCVVyRr1qBZY807AaSJkOQ4ffBowHopRiTOYXiIIS8Nwv9wcIY834_hnqxRHleYlmKPTGTGRLFXyJ6PL9GsID5NCV6ERpAiwhz7eQGdqh8H6o5aWQS2ujogEXM5JtUk6c5-xgn227GGnVifWrBqBi8N5hE0B9tHUbimaXMMKSuNowinDXnXtOwVWNjdGbuecATi-wi8P6p6n37AzK6935xCIKku--0ncRT6r2izy-SZ2EEaSBojaX1M0tB_ZOiQPMTphm8JDQydMfT-7Q3awI4VyQNV1qQw0ziOILZ3-xPyMlSQAKcowM84ihPGBEh9xTYlODr195Igm5BAzWbGQL04hNKPoGAkmqBsFb1JLjmIYaY_KtWOSvW65XZL87xzAvrcmXnRPdEOTs16lZta9WJBZoJMMXPMsQv7BOGZCuc_4vRpm6UqtQENTOylio0hea3CDaZB1hCJIg4ZdC8QRfDxusBTc1k2hfwLXy7DWkUmUraoiGLyEK5lsTM151euM1XgwjEFJuG9DFlZC8tNFWjlgDxOYln-wgKd_XLAUxJuqoH6wsKa_XJImm3qcp3JdS7fPO4NYTC5IeTRfV0-ZPKhKJz8SkUnhQvb4tX-3xlo-Bx66EueJ-itEkdid6BIS-1_g6xCgeDzsWXOoCSQsdv6gKLnBJJFFGXgEMRLtywMsO-lOexvrDEOMvQ21WodyOXHCqwbxevXXQ0CPSXe2qOlTF4K_UePrKGBKra2F2P0Rr0kB6y0_HeFxsO5Gtxr7bYfVetG0zttdqrRareTooEaiwdDEkp2eKlmPasJaN7OTwRqbYF6W6DRFjisRdqBeXP_t0GykG8HraxUZoe2QL0t0GgLHLYBSguqW9GlfupGO-9G-9yNdtGJpg260dRuNK0brVv-0LpV5lq3VaJ1WyXaeffHr12VMDIrpQVsKyYUvRn6HZtX6E_IHFdo7IibifOH1KN55cK3b43m1bJ42zdQoPIN81eXkuvnDhJq7Eg4GK0GpR-GGi8EQhkv2LhkRh_HjzCcLcORGc6iztjWZ036TWT9Js7-U4AYb-IIhDTUxDk6VOUPkG3ehV1h8YKeSUVB76nGIAj2ntgsVXpKbl2CgBdbY4UfTsM7J-EnTuNq_9ypgNAuOtH0QTfaKcVRPe6aPAfbEbXUVOuPluZHldlqOV4tak47DNvz1ztQpy202Ut6tzJP17vRjG60YQta1TtSXLGO6CGTz89BaDFeZDhVksgjuBLM1PLMgmqDdSV9Pkn6ePdq0OJsZiZp1DjmTFX8PUcJJYS8OObI8xY6NdA-t5qKnLLoQ-8sBVt-8YqoMQiYkixhMOwXSOdLZUGtzSZgO0egtweA1fPd99DSBLvlE6NbPjFOySfVs4bytKF23uDASp3gZ5deEdzI7z_AG86U3zpTeIqXN8xpCYK8Qy12k2PFKWj5z5uZwaiArOvtk6Z2aY5ay5UmUfTTKUZLimS8rdFWjynG14vFOE6xq8PN9oSGtjQf09yspmPLnrpwVUZ945pDj57smNeMQdb9YRu4GIEad6myl0nftNq-wHukCPMa0gHDEJEXqttCBVmUyOJo3jKGnXKE8akb7fyk1FIJcGGqA1I-98Yxuz1RGW2fqKTFZo1vLGfiWso4ioug9LN4ecByFDiXg-prezhovZ9BiN_g9TZTbFe43BiEKWarDa3EUqKHlPMyMe-OKbdNpbZlnOywLFj2cPNNEi0T7Lv0hh2ss8N4hbaFD6Ff4sw4pwgXrigsoTiQxMvcI0yc5V55rF6KK1qDBfe0O6A0QEsFq-rSpcI1qqjHm6km28TB31XK44u6nftHRu0Nv0XSNS4jSzUa3g9wgNCn9ohQkVMtG94JSGup3HHE4hk05NKh1gKjt8AYTZhtZPTOehucbrww6F32ftLmu17-CGvgrncJt-Dup7veGW-nf27huZWK8rQA1_4CepEEXo6nQZjHae-Stp_1vCKPl6_E710-eFGGS9Ak9CASNqL11_-INcE4">link</a>.</p> <h2 data-number="3.1" id="stop-the-talking-and-show-us-some-code"><span class="header-section-number">3.1</span> Stop the talking and show us some code!</h2> <h3 data-number="3.1.1" id="installation"><span class="header-section-number">3.1.1</span> Installation</h3> <p>Well, if you follow the manual, you will get there.</p> <p>At the time I tested things, only Ubuntu 18.04 was supported. Later versions will fail with a nf_conntrack issue, so you may need to run everything in a VM.</p> <p>Running things in a VM can also create problems, but more on that later.</p> <pre><code># Check if you have virtualization support
sudo modprobe kvm
lsmod | grep kvm

# Check if you have AVX2 (a flavor of SIMD for floating point operations)
egrep &quot;(avx2)&quot; /proc/cpuinfo

# Check if you have virtualization instructions
egrep &quot;(svm|vmx)&quot; /proc/cpuinfo

# Don&#39;t even try to continue if one of the
# previous checks didn&#39;t return something

# Clone Riab
git clone &quot;https://github.com/onosproject/sdran-in-a-box.git&quot;

# Install Vagrant (VM manager) and the libvirt extension
apt install vagrant qemu-kvm libvirt-daemon-system

# Create a new Ubuntu 18.04 VM
vagrant init &quot;generic/ubuntu1804&quot; # bionic64

# Edit the vMemory and vCpus for the machine
# If you use the emulated data-plane, use &gt;22GB of RAM
#
#  # -*- mode: ruby -*-
#  # vi: set ft=ruby :
#  Vagrant.configure(&quot;2&quot;) do |config|
#    config.vm.box = &quot;generic/ubuntu1804&quot; #
#    config.vm.synced_folder &quot;./&quot;, &quot;/vagrant&quot; # sync the local directory to the VM&#39;s /vagrant directory
#    config.vm.provider &quot;libvirt&quot; do |vb|
#      #vb.gui = true
#      vb.cpus = 14
#      vb.memory = &quot;21384&quot;
#    end
#    config.ssh.insert_key=false # passwordless login
#  end

# Start the VM
vagrant up

# Log into the VM
vagrant ssh

# Update packages
sudo apt-get update
sudo apt-get upgrade

# Lock the kernel version. It might not be a problem on every cloud,
# but it was on mine.
sudo apt-mark hold linux-generic linux-image-generic linux-headers-generic

# Install extra kernel modules (nf_conntrack)
sudo apt install linux-modules-extra-*-generic
sudo modprobe nf_conntrack
sudo apt-get install make net-tools

# Nagivate to the cloned SD-RAN-in-a-box directory
cd /vagrant/sdran-in-a-box</code></pre> <p>Now, how do we do anything?</p> <h3 data-number="3.1.2" id="basic-helmkubernetes-background-information"><span class="header-section-number">3.1.2</span> Basic Helm/Kubernetes background information</h3> <p>I’m definitely not an specialist in SD-RAN, Kubernetes or ONOS, so it took some experimentation to get anything done.</p> <p><a href="https://helm.sh/docs/intro/using_helm/">Helm charts</a> are used as an encapsulation for kubernetes applications along which can be configured using templates for the different settings. It is just as bad as it sounds.</p> <p>Helm charts can have subdirectories, which are considered subdependencies and are installed recursively.</p> <p>A helm chart can be installed from a local repository (so you can work on a local clone)</p> <pre><code>helm install oai-enb-du sdran-in-a-box/workspace/sdran-helm-charts/oai-enb-du</code></pre> <p>The templated settings allow for the substitution of values via the command line or via yaml files (much preferred).</p> <pre><code>echo &#39;{
 config.oai-enb-du.mode: &quot;nfapi&quot;,
 config.oai-enb-du.radio.eutra_band: 42,
 config.oai-enb-du.radio.rbs: 100,
 config.oai-enb-du.radio.tx_gain: 90,
 config.oai-enb-du.radio.rx_gain: 125,
 config.oai-enb-du.radio.nb_antennas_rx: 2,
 config.oai-enb-du.radio.nb_antennas_tx: 2
}&#39; &gt; values.yaml
helm install -f values.yaml oai-enb-du sdran-in-a-box/workspace/sdran-helm-charts/oai-enb-du</code></pre> <p>In this case, one of the templates in sdran-in-a-box/workspace/sdran-helm-charts/oai-enb-du/templates/*.tpl will have their field values replaced with those in the yaml settings file.</p> <p>For example, in the values file, we set "oai-enb-du.mode" to "nfapi", which will be processed by the following template:</p> <pre><code>{{ $duMode := index .Values &quot;config&quot; &quot;oai-enb-du&quot; &quot;mode&quot; }}</code></pre> <p>and transformed into the following line</p> <pre><code>duMode := &quot;nfapi&quot;</code></pre> <p>The helm charts used by the SD-RAN-in-a-box are installed by the makefiles em sdran-in-a-box/mk, using the following commands</p> <pre><code>helm upgrade --install $(HELM_ARGS) \
     --namespace $(RIAB_NAMESPACE) \
     --values $(HELM_VALUES) \
      ...</code></pre> <p>The values of HELM_VALUES are read from the default settings file sdran-in-a-box/sdran-in-a-box-values.yaml</p> <p>To install from your own sdran-helm-charts fork, replace the repository link in sdran-in-a-box/Makefile. To do that, search for</p> <pre><code>            $(ONOS_GITHUB_URL)/sdran-helm-charts</code></pre> <p>The clone is only done if the directory does not exist, so you can modify it directly, then commit and push changes later. Just remember to do that before running "make clean-all", otherwise the changes will be lost.</p> <h3 data-number="3.1.3" id="installing-riab-with-data-plane-emulation"><span class="header-section-number">3.1.3</span> Installing RIAB with data-plane emulation</h3> <p>If you do not have enough RAM, the VM or your computer will crash due to OOM.</p> <pre><code># Install RIAB with OAI emulation
make OPT=oai

# Execute tests to check if the data-plane is actually working
# Tests are located in sdran-in-a-box/mk/test-cases
# E2E tests using pings in the user data-plane
make test-user-plane

# E2E tests using iperf in the user data-plane
make test-rsm-dataplane</code></pre> <p>Everything should be working now. You must be thinking on passing quite some traffic through that emulated network right now, but it isn’t that simple.</p> <p>At least on my computer, due to using a normal kernel (not the low-latency one), and due to low CPU clocks (1st gen Ryzen 1700), the synchronization between the UEs and the eNB is lost quite frequently. I remember looking at the ONF Slack channel for answers, but it was still being investigated.</p> <p>Due to that issue, I had to take everything down and up again, which took quite a lot of time. I tested the commands suggested by Woojoong Kim (I believe he is the lead developer for Riab), but didn’t really work for my specific case.</p> <pre><code># I only really use clean-all, because it always works. Beware: it is super slow and deletes everything

# It deletes ONOS RIC services and RAN-Simulator but Kubernetes is still running
make reset-test

# Deletes Kubernetes environment; Eventually, all ONOS RIC and
# RAN-Simulator are terminated; The Helm chart directory is not deleted

make clean

# Deletes everything, including the Kubernetes environment,
# all componentes/PODs which RiaB deployed, and even the Helm chart directory

make clean-all</code></pre> <h3 data-number="3.1.4" id="other-useful-commands"><span class="header-section-number">3.1.4</span> Other useful commands</h3> <p>Other useful commands I’ve learned while testing these.</p> <pre><code># Lists all kubernetes namespaces and services
kubectl get po --all-namespaces

# Lists kubernetes services in the riab namespace
kubectl -n riab get pods

# Lists the status of sd-ran
helm -n riab status sd-ran

# Lists all sd-ran settings
helm -n riab get all sd-ran

# Check the status of different pods running SD-RAN services
watch kubectl -n riab get pods

# Comand to get the kubernetes pod shell to access binaries
kubectl -n riab exec -it $(kubectl -n riab get pods -l type=cli -o name) -- /bin/sh</code></pre> <h2 data-number="3.2" id="thats-enough-code.-now-show-us-something-interesting"><span class="header-section-number">3.2</span> That’s enough code. Now show us something interesting</h2> <p>I wish I had some banging xApps up and running... Maybe getting to work in real networks with people from Mavenir, Parallel, Radisys or Nokia...</p> <p>But in my excitement about this, I ended up talking too much and getting a ton of people wanting to get involved, which ended up drastically slowing down the process.</p> <p>A few partners offered us their clouds, which I’m very grateful for, but they were quite up to the task of running real-time services. Some failed to setup Riab due to missing AVX2 instructions, others due to disabled smx/vmx instructions for security reasons, others had very slow drives which timed out some of the services and prevented Riab from getting setup. Even if they worked, I’m not sure their clocks and number of cores would be able to keep up with the emulated network.</p> <p>I also had my fair of issues trying to understand Helm packaging.</p> <p>The best I managed to do was adapt our in-house traffic injection tool, <a href="https://github.com/notopoloko/Eros">Eros</a>, and use it to <a href="https://github.com/Gabrielcarvfer/sdran-in-a-box/blob/work/inject_eros_traffic.py">inject</a> some network traffic and see how things behaved. We produced the Figure <a href="#fig:traffic-injection" data-reference-type="ref" data-reference="fig:traffic-injection">2</a>, showing the behavior of the UE downlink when streaming a MPEG DASH-like traffic from the remote serve.</p> <figure> <img src="/assets/img/2022-12-22-experimental-platforms-for-open-ran/traffic-injection-sdran.png" id="fig:traffic-injection" style="width:100.0%" alt=""/><figcaption>Traffic injection from the remote server to the UE using the emulated data-plane</figcaption> </figure> <h1 data-number="4" id="next-steps"><span class="header-section-number">4</span> Next steps</h1> <p>For now, I’ve stopped playing with SD-RAN and started focusing on my own O-RAN implementation for the ns-3 simulator.</p> <p>It is kind of sad that I decided to change course too late and missed the <a href="https://www.comsoc.org/publications/journals/ieee-jsac/cfp/open-ran-new-paradigm-open-virtualized-programmable-and">journal</a> I had in mind.</p> <p>Hopefully it will make it to the Brazilian Symposium of Computer Networks and Distributed Systems (SBRC), which funnily enough will be hosted by my alma mater, in the next year.</p> <p>The source code for my O-RAN module should be published in a month or so, before I finish the paper for the tools section of the SBRC. It still needs some refactoring to look decent.</p> <h1 class="unnumbered" data-number="" id="references">References</h1> <div id="refs" class="references hanging-indent" role="doc-bibliography"> <div id="ref-Solis_2022"> <p>Solis, Priscila, Gabriel C Ferreira, Raimundo Guimarães Saraiva Jr., and Paulo Henrique Portela de Carvalho. 2022. “Uma análise Do Open RAN E Os Desafios Da Sua Experimentação Em Plataformas de código Aberto.” In <em>Anais Do XL Simpósio Brasileiro de Telecomunicações E Processamento de Sinais</em>. Sociedade Brasileira de Telecomunicações. <a href="https://doi.org/10.14209/sbrt.2022.1570824548">https://doi.org/10.14209/sbrt.2022.1570824548</a>.</p> </div> </div>]]></content><author><name>Gabriel</name></author><summary type="html"><![CDATA[1 Introduction The Open RAN (O-RAN) architecture and the O-RAN Alliance specifications have the potential to promote virtualized and disaggregated Radio Access Networks (RANs), connected through open interfaces and optimized by intelligent controllers. Understanding the O-RAN architecture, interfaces and workflows is critical to the future of the telecommunications ecosystem. The evolution of O-RAN has many challenges due to its complexity, requiring experimental platforms to evaluate programmable and virtualized protocol stacks with the new open interfaces, in conjunction with RAN optimization within the 5G architecture and beyond. Our team at UnB spent quite some time looking for different platforms for experimentation and presented our findings, (Solis et al. 2022), at the Brazilian Symposium of Telecommunications (SBRT2022). Since I didn’t have any free time up till now and we didn’t have enough space in the paper, lets review things here, were we can put any number of images. 2 What do we need to get a working Open-RAN setup? It isn’t an official architecture diagram, but the diagram of the reference implementation made by the O-RAN Software Community (O-RAN SC). Figure 1 shows the main components of an O-RAN setup, which are: the 3GPP core: 4G and 5G are supported in the O-RAN specifications (some companies are apparently adapting older networks too) the RAN components: the eNB or gNB, or its disaggregated counterparts (CU, DU, RU) the O-RAN components: near-RT RIC, non-RT RIC and the rest of the SMO, plus SDK for xApp/rApp development Architecture diagram of the reference O-RAN implementation (the drawings are mine) Let’s look at each components alternatives. 2.1 5G Core implementations We found two open-source implementations that are frequently used. The first one is the famous OpenAirInterface-5G Core Network (OAI-5G CN), which has been in the works for years and is used by a lot of researchers and companies, in both private and public networks. The second one is the free5gc, which started as a fork of a 4G Core, NextEPC, and continues to evolve. Also seems to be used on public networks. There are quite a lot of proprietary cores, but we can’t be certain those are not forks of the open-source ones. To name a few companies and their cores: Ericsson - 5GC Nokia - 5GC Tecore – iCore Altran/Capgemini Engineering - Virtualized Next Generation 5G Core (ViNGC) Mavenir – Mavenir Converged Package Core (MAVCore) Huawei – SDM/PCC After we have a 3GPP core, we can then connect it to our RAN. 2.2 RAN implementations Most likely due to complexity, performance issues due to signal processing overhead, and standards conformance and interoperability testing, there seems to be less options in terms of RAN. The eNB/gNB/CU/DU are available in open-source projects such as: OpenAirInterface SrsRAN Radisys – gNB DU (part of the reference O-RAN SC implementation) The O-RAN RICs and interfaces are available in open-source projects such as: OAI - FlexRic Open Networking Foundation (ONF) - SD-RAN Other proprietary RAN implementations available: Radisys – ConnectRAN Huawei – SingleRAN Ericsson – Cloud RAN Nokia – AirScale Cloud RAN And it is at this point someone asks me why I included Huawei in this list. The answer is: they can probably adapt their solution easily to support Open-RAN. As the time goes by, it will get even easier since all the tooling required for testing and conformance will be in place. If O-RAN really works and is set as a requirement in new spectrum auctions, they will certainly do it. 2.3 Service Management and Orquestration (SMO) This is where things really start to get big and ugly. We have great SMO services such as Magma, which was started by Facebook, then moved to Telecom Infrastructure Project (TIP), and finally to Linux Foundation (stop moving the damned project around). Too bad it isn’t comparible with O-RAN. Even then, it probably is one of the reasons O-RAN even became a thing, since TIP is also responsible for pushing O-RAN forward, along with Telcos, software/hardware/cloud partners and some governments. There is also Open-Source MANO, which doesn’t really seem to have gained much traction on the O-RAN space. But maybe I’m badly informed. Kubernetes on the other hand, seems to be the popular choice, since both Trirematics, OAI’s SMO solution, and the SD-RAN, ONF’s SMO solution, are based on Kubernetes. All of the services are implemented as Virtualized Network Functions (VNFs), that are executed within containers. The containers are glued together by software defined networks (SDNs). SD-RAN uses ONF’s in-house SDN/NFV solution named ONOS, while the reference O-RAN implementation uses Linux Foundation’s Open Network Automation Platform (ONAP). 2.4 A blueprint With all components at hand, we then need to sewn them together. This is when the problems really start appearing for a novice. There are quite a lot of components, all of them with different configuration settings, syntaxes and semantics. You have no idea what most of the settinds do and how to properly set them up, but you’re still supposed to figure it out... You probably won’t, because available documentation is pretty bad. I admit my documentation isn’t up to par either, but that’s how things are. We are supposed to believe Akraino blueprints work, but I haven’t got a single one to work. Seems like another dead project from the Linux’s Foundation. ONF is the most friendly one, with a complete and working blueprint, which they named SD-RAN-in-a-box , or just Riab. 3 SD-RAN-in-a-box: a not so hidden gem By far, SD-RAN-in-a-box is the most user-friendly Open-RAN solution. You just run a few commands and it setups everything up using the magic of good old Bash, Ansible, Kubernetes and ONOS. The user can select if they want to emulate the user data-plane. Which of course we want. It will create VM’s running OAI that act like the eNB and UE, connected by a virtual channel. More surprisingly, since OAI is already running anyways, the user can choose to use a Software Defined Radio (SDR) to get a real network going. Of course the settings are meant for a lab setup, with very few UEs. There is a popular saying that says that the devil is in the details. I’d say that the devil is in the implementations. While the O-RAN standard look fairly simple, the implementation complexity can vary wildly depending on the implementer choices. When I found SD-RAN-in-a-box, I started looking at the settings to understand how exactly it worked, what was connected to what, which standard defined the different interfaces. Ended up writing a MermaidJS diagram. But since is too big to show, here is the link. 3.1 Stop the talking and show us some code! 3.1.1 Installation Well, if you follow the manual, you will get there. At the time I tested things, only Ubuntu 18.04 was supported. Later versions will fail with a nf_conntrack issue, so you may need to run everything in a VM. Running things in a VM can also create problems, but more on that later. # Check if you have virtualization support sudo modprobe kvm lsmod | grep kvm]]></summary></entry><entry><title type="html">ns-3 transition from waf to CMake</title><link href="/2021/11/29/ns3-waf-to-cmake-transition.html" rel="alternate" type="text/html" title="ns-3 transition from waf to CMake"/><published>2021-11-29T00:00:00+00:00</published><updated>2021-11-29T00:00:00+00:00</updated><id>/2021/11/29/ns3-waf-to-cmake-transition</id><content type="html" xml:base="/2021/11/29/ns3-waf-to-cmake-transition.html"><![CDATA[<h1 class="unnumbered" data-number="" id="introduction">Introduction</h1> <p>The ns-3 network simulator is one of the most used network simulators in the academia, however, it has a very steep learning curve, especially when you are still learning C++ and the standards themselves.</p> <p>Not their fault in any way, it is just that modeling complex systems is hard by itself.</p> <p>Ever-evolving standards, adding more and more features as the time goes while keeping backwards compatibility also doesn’t help.</p> <p>What can we do to help?</p> <p>Better guides? Maybe, but I do not think they help as much. If it was something interactive, it could be pretty cool. I have tried setting up Jupyter notebooks for more interactive guides using the power of <a href="https://github.com/jupyter-xeus/xeus-cling">Cling-Xeus</a>.</p> <p>Better examples? Maybe. Some examples are very simple while we have much more complicated scenarios in published papers.</p> <p>Better docs? I personally don’t think they help as much when you’re already lost.</p> <p>Maybe include a parser to be able to describe the simulations from a higher level description language like other simulators do? That does not sound like a bad idea, since setting up some things can be quite complicated if you are just getting started.</p> <p>Better tooling? This is where I come in.</p> <h1 class="unnumbered" data-number="" id="build-systems">Build systems</h1> <p>Something so simple as a build system should not be such a problem, am I right? Well, it is not if you already know what you are doing, how to get things done, etc.</p> <p>If you are part of the group just getting started into everything at once, it is overwhelming and your productivity is basically none for a while, which can be frustrating. I was one of these.</p> <p>So, how bad can it really be? Well, pretty bad. Since I have been used to IDEs, where projects can be organized neatly, settings are managed graphically, auto-completion works, coding mistakes are properly highlighted, you can easily navigate and refactor the code and my preferred feature: VISUAL DEBUGGING.</p> <p><img src="/assets/img/2021-11-29-ns3-waf-to-cmake-transition/visual_debugging.png" style="width:100.0%" alt="image"/></p> <p>Yes, I like visual debuggers. Really like them. For a few reasons, being the following two the main ones:</p> <ul> <li><p>I can see the values of the variables not only in the memory watches panels, but inlined into the code</p></li> <li><p>I can see the callstack</p></li> </ul> <p>But you may ask me: <em>-What the devil build systems got to do with visual debugging?</em>.</p> <p>And that is a pretty good question. While build systems do not prevent you from using visual debuggers, it limits the amount of information and insights you can get from the code. Some IDEs, for example, do not even allow for that.</p> <p>Why all of this matters? <a href="https://waf.io/"><strong>WAF</strong></a>.</p> <p>Waf is a pretty cool build system written entirely in Python and is supper easy to distribute with your project, so it needs only Python installed (which you would probably already have anyways) and whatever your project needs to get build (e.g. compiler).</p> <p>ns-3 have been using Waf for a while, and it has been working well for quite a long time, which goes to show it is not a bad build system in any way.</p> <p>My grudges with Waf is just that it does not support IDEs and the organization in different steps, plus how things are organized seem overly complicated for me. What I dislike the most however is that it does not produce intermediary build system files that I can inspect visually and discover what I am doing wrong.</p> <p>So I started working on CMake support...</p> <h1 class="unnumbered" data-number="" id="ns-3-with-cmake">ns-3 with CMake</h1> <p>Since I started my masters, my advisor said I should start working with ns-3. If it was not for that, I would probably be using ns-2 or OMNET++.</p> <p>Anyways, so I started rewriting everything ns-3 needed on CMake. Took quite some time to write everything, since I was not only translating Waf, but "reverse engineering". I admit this was pretty stupid, but I simply could not understand how Waf did its magic (if it had written makefiles or something like that, it would have been easier and faster).</p> <p>Since 2018 I have been gathering feedback and refactoring everything to <a href="https://gitlab.com/nsnam/ns-3-dev/-/merge_requests/460">upstream that support</a>, which seems like it is going to happen soon (release 3.36).</p> <p>With CMake, all major IDEs are supported from the get go: Microsoft <a href="https://gabrielcarvfer.github.io/NS3/installation/visualstudio">Visual Studio</a> and <a href="https://gabrielcarvfer.github.io/NS3/installation/visualcode">Visual Code</a>, <a href="https://gabrielcarvfer.github.io/NS3/installation/clion">Jetbrains CLion</a>, <a href="https://gabrielcarvfer.github.io/NS3/installation/xcode">Apple XCode</a>, <a href="https://gabrielcarvfer.github.io/NS3/installation/codeblocks">Code::Blocks</a>, <a href="https://gabrielcarvfer.github.io/NS3/installation/eclipse">Eclipse</a>, CodeLite and others.</p> <p>Of course CMake can also be called directly via <a href="https://gabrielcarvfer.github.io/NS3/installation/terminal">the command-line</a>, but commands are very verbose.</p> <p>Command-line users will be happy to know they were not forgotten with the <a href="https://gabrielcarvfer.github.io/NS3/installation/ns3waf">ns3</a> wrapper script for CMake (previously known as fakewaf and ns3waf). The script provides pretty much the same functionality expected from Waf, including exporting library paths, making programs just work instead of having to export the path first.</p> <p><strong>My suggestion</strong>: use the ns3 wrapper when you need it. For example, when using IDEs that do not natively support CMake projects such as Xcode, Code::Blocks and Eclipse, you will need to specify the proper CMake Generator to get a project for them. Also, you will need to close the IDE, refresh the CMake cache manually (re-run ’ns3 configure’) whenever you add/remove a source file, create a new module or add a new dependency (either module or library), then you can open the IDE once again and continue using. I know it is pretty bad and that is why I suggest either CLion (my preferred IDE) or VsCode, which works surprisingly well with CMake.</p> <h2 class="unnumbered" data-number="" id="what-changes-for-a-module-developer">What changes for a module developer?</h2> <p>Not that much. I have purposefully made it very similar to make the transition as smooth as possible. Copy and paste source lists to the appropriate variables, then clean semicolons and quotes and you are good to go if your module does not have any special needs (e.g. external library or options).</p> <p><img src="/assets/img/2021-11-29-ns3-waf-to-cmake-transition/modules_transition.png" style="width:100.0%" alt="image"/></p> <p>If you need user-provided paths to look for libraries, you can follow example from the brite:</p> <pre><code>    # This part is similar the conf part in a wscript
    # I know it is pretty confusing, but this is due to CMake limitations
    #
    # First we declare/set a variable that points to a PATH in the CMake CACHE
    #    (a.k.a. this value won&#39;t be lost accross runs)
    # Then we declare/set a variable also stored in the cache,
    #    but now it is used INTERNALly by us to determine if BRITE was
    #    found in the NS3_WITH_BRITE path
    set(NS3_WITH_BRITE &quot;&quot; CACHE PATH &quot;Build with brite support&quot;)
    set(NS3_BRITE &quot;OFF&quot; CACHE INTERNAL &quot;ON if Brite is found in NS3_WITH_BRITE&quot;)

    # If a path for Brite was not given, we just skip the entire module
    #     and pretend it does not exist
    if(NOT NS3_WITH_BRITE)
      return()
    endif()

    # If a path for Brite was given, we search for both the library and
    #     the header in the NS3_WITH_BRITE folder
    find_library(brite_dep brite
                 PATHS ${NS3_WITH_BRITE}
                 PATH_SUFFIXES /build /build/lib /lib
                 )
    find_file(brite_header Brite.h
              HINTS ${NS3_WITH_BRITE}
              PATH_SUFFIXES /build /build/include /include
              )

    # If both are not found, we return a message indicating it is the
    #     case and stop processing this module by returning to the src folder
    if(NOT (brite_dep AND brite_header))
      message(STATUS &quot;Brite was not found in ${NS3_WITH_BRITE}&quot;)
      return()
    endif()

    # If both were found, we get the directory containing
    #     the brite header and use it as an include folder
    get_filename_component(brite_include_folder ${brite_header} DIRECTORY)
    include_directories(${brite_include_folder})

    # We also set the NS3_BRITE variable, indicating it was found
    set(NS3_BRITE &quot;ON&quot; CACHE INTERNAL &quot;ON if Brite is found in NS3_WITH_BRITE&quot;)

    # your module name (in this case brite, which can be used by other libraries
    #    including ${libbrite} in their libraries_to_link variable)
    set(name brite)

    # your source files
    set(source_files helper/brite-topology-helper.cc)

    # your header files
    set(header_files helper/brite-topology-helper.h)

    # link to dependencies
    set(libraries_to_link
            ${libnetwork} # notice the ns-3 modules required by libbrite
            ${libcore}
            ${libinternet}
            ${libpoint-to-point}
            ${brite_dep} # notice the brite_dep library
            )

    # your test source files
    set(test_sources test/brite-test-topology.cc)

    # The magic macro that does Waf-like magic
    build_lib(&quot;${name}&quot;
              &quot;${source_files}&quot;
              &quot;${header_files}&quot;
              &quot;${libraries_to_link}&quot;
              &quot;${test_sources}&quot;
              )</code></pre> <p>If you need to find a library with a FindPackage(), the config-store module serves as an example:</p> <pre><code>
    # Search for HarfBuzz and GTK3
    #     (we currently do this in ns-3-dev/buildsupport/macros_and_definitions.cmake)

    # Only check for GTK3 if the user set this flag
    #     (currently in ns-3-dev/CMakeLists.txt)
    if(${NS3_GTK3})
      find_package(HarfBuzz QUIET)
      if(NOT ${HarfBuzz_FOUND})
        message(STATUS &quot;Harfbuzz is required by GTK3 and was not found.&quot;)
      else()
        set(CMAKE_SUPPRESS_DEVELOPER_WARNINGS 1 CACHE BOOL &quot;&quot;)
        find_package(GTK3 QUIET)
        unset(CMAKE_SUPPRESS_DEVELOPER_WARNINGS CACHE)
        if(NOT ${GTK3_FOUND})
          message(STATUS &quot;GTK3 was not found. Continuing without it.&quot;)
        else()
          message(STATUS &quot;GTK3 was found.&quot;)
        endif()
      endif()
    endif()

    # Search for LibXml2
    #     we currently do this in buildsupport/macros_and_definitions.cmake)
    find_package(LibXml2 QUIET)
    if(NOT ${LIBXML2_FOUND})
      message(STATUS &quot;LibXml2 was not found. Continuing without it.&quot;)
    else()
      message(STATUS &quot;LibXml2 was found.&quot;)
      add_definitions(-DHAVE_LIBXML2)
    endif()

    set(name config-store)

    # add optional sources if an optional GTK3 dependency is found
    if(${GTK3_FOUND})
      set(gtk3_sources model/display-functions.cc
                       model/gtk-config-store.cc
                       model/model-node-creator.cc
                       model/model-typeid-creator.cc
      )

      set(gtk3_headers model/gtk-config-store.h)
      include_directories(${GTK3_INCLUDE_DIRS} ${HarfBuzz_INCLUDE_DIRS})
      set(gtk_libraries ${GTK3_LIBRARIES})
    endif()

    # add optional sources if an optional LibXml2 dependency is found
    if(${LIBXML2_FOUND})
      set(xml2_sources model/xml-config.cc)
      set(xml2_libraries ${LIBXML2_LIBRARIES})
      include_directories(${LIBXML2_INCLUDE_DIR})
    endif()

    # add optional sources to the source_files list
    set(source_files
        ${gtk3_sources}
        ${xml2_sources}
        model/attribute-default-iterator.cc
        model/attribute-iterator.cc
        model/config-store.cc
        model/file-config.cc
        model/raw-text-config.cc
    )

    # add optional header to the header_files list
    set(header_files ${gtk3_headers} model/file-config.h model/config-store.h)

    # add optional libraries to the libraries_to_link list
    set(libraries_to_link ${libcore} ${libnetwork} ${xml2_libraries} ${gtk_libraries})

    set(test_sources)

    build_lib(&quot;${name}&quot;
              &quot;${source_files}&quot;
              &quot;${header_files}&quot;
              &quot;${libraries_to_link}&quot;
              &quot;${test_sources}&quot;
              )
</code></pre> <p>You may ask: <em>-Is there yet another way of finding libraries?</em>. And the answer could not be other than: of course there is.</p> <p>For libraries that use pkg-config, you can follow the fd-net-device example.</p> <pre><code>    # Include CMake script to use pkg-config
    include(FindPkgConfig)
    # If pkg-config was found, search for dpdk
    if(PKG_CONFIG_FOUND)
      pkg_check_modules(DPDK libdpdk)
    endif()
    ...
    # Reset cached variable
    set(ENABLE_DPDKDEVNET False CACHE INTERNAL &quot;&quot;)
    ...
    # Set cached variable if both pkg-config and libdpdk are found
    if(PKG_CONFIG_FOUND AND DPDK_FOUND)
      set(ENABLE_DPDKDEVNET True CACHE INTERNAL &quot;&quot;)
    endif()</code></pre> <h1 class="unnumbered" data-number="" id="the-end">The end</h1> <p>Feel free to provide feedback either on the <a href="https://groups.google.com/g/ns-3-users">ns-3-users group</a>, via the <a href="https://gitlab.com/nsnam/ns-3-dev/-/merge_requests/460">open MR</a>.</p> <p>You can also find me on the <a href="https://ns-3.zulipchat.com/">ns-3 Zulip</a>.</p>]]></content><author><name>Gabriel</name></author><summary type="html"><![CDATA[Introduction The ns-3 network simulator is one of the most used network simulators in the academia, however, it has a very steep learning curve, especially when you are still learning C++ and the standards themselves. Not their fault in any way, it is just that modeling complex systems is hard by itself. Ever-evolving standards, adding more and more features as the time goes while keeping backwards compatibility also doesn’t help. What can we do to help? Better guides? Maybe, but I do not think they help as much. If it was something interactive, it could be pretty cool. I have tried setting up Jupyter notebooks for more interactive guides using the power of Cling-Xeus. Better examples? Maybe. Some examples are very simple while we have much more complicated scenarios in published papers. Better docs? I personally don’t think they help as much when you’re already lost. Maybe include a parser to be able to describe the simulations from a higher level description language like other simulators do? That does not sound like a bad idea, since setting up some things can be quite complicated if you are just getting started. Better tooling? This is where I come in. Build systems Something so simple as a build system should not be such a problem, am I right? Well, it is not if you already know what you are doing, how to get things done, etc. If you are part of the group just getting started into everything at once, it is overwhelming and your productivity is basically none for a while, which can be frustrating. I was one of these. So, how bad can it really be? Well, pretty bad. Since I have been used to IDEs, where projects can be organized neatly, settings are managed graphically, auto-completion works, coding mistakes are properly highlighted, you can easily navigate and refactor the code and my preferred feature: VISUAL DEBUGGING. Yes, I like visual debuggers. Really like them. For a few reasons, being the following two the main ones: I can see the values of the variables not only in the memory watches panels, but inlined into the code I can see the callstack But you may ask me: -What the devil build systems got to do with visual debugging?. And that is a pretty good question. While build systems do not prevent you from using visual debuggers, it limits the amount of information and insights you can get from the code. Some IDEs, for example, do not even allow for that. Why all of this matters? WAF. Waf is a pretty cool build system written entirely in Python and is supper easy to distribute with your project, so it needs only Python installed (which you would probably already have anyways) and whatever your project needs to get build (e.g. compiler). ns-3 have been using Waf for a while, and it has been working well for quite a long time, which goes to show it is not a bad build system in any way. My grudges with Waf is just that it does not support IDEs and the organization in different steps, plus how things are organized seem overly complicated for me. What I dislike the most however is that it does not produce intermediary build system files that I can inspect visually and discover what I am doing wrong. So I started working on CMake support... ns-3 with CMake Since I started my masters, my advisor said I should start working with ns-3. If it was not for that, I would probably be using ns-2 or OMNET++. Anyways, so I started rewriting everything ns-3 needed on CMake. Took quite some time to write everything, since I was not only translating Waf, but "reverse engineering". I admit this was pretty stupid, but I simply could not understand how Waf did its magic (if it had written makefiles or something like that, it would have been easier and faster). Since 2018 I have been gathering feedback and refactoring everything to upstream that support, which seems like it is going to happen soon (release 3.36). With CMake, all major IDEs are supported from the get go: Microsoft Visual Studio and Visual Code, Jetbrains CLion, Apple XCode, Code::Blocks, Eclipse, CodeLite and others. Of course CMake can also be called directly via the command-line, but commands are very verbose. Command-line users will be happy to know they were not forgotten with the ns3 wrapper script for CMake (previously known as fakewaf and ns3waf). The script provides pretty much the same functionality expected from Waf, including exporting library paths, making programs just work instead of having to export the path first. My suggestion: use the ns3 wrapper when you need it. For example, when using IDEs that do not natively support CMake projects such as Xcode, Code::Blocks and Eclipse, you will need to specify the proper CMake Generator to get a project for them. Also, you will need to close the IDE, refresh the CMake cache manually (re-run ’ns3 configure’) whenever you add/remove a source file, create a new module or add a new dependency (either module or library), then you can open the IDE once again and continue using. I know it is pretty bad and that is why I suggest either CLion (my preferred IDE) or VsCode, which works surprisingly well with CMake. What changes for a module developer? Not that much. I have purposefully made it very similar to make the transition as smooth as possible. Copy and paste source lists to the appropriate variables, then clean semicolons and quotes and you are good to go if your module does not have any special needs (e.g. external library or options). If you need user-provided paths to look for libraries, you can follow example from the brite: # This part is similar the conf part in a wscript # I know it is pretty confusing, but this is due to CMake limitations # # First we declare/set a variable that points to a PATH in the CMake CACHE # (a.k.a. this value won&#39;t be lost accross runs) # Then we declare/set a variable also stored in the cache, # but now it is used INTERNALly by us to determine if BRITE was # found in the NS3_WITH_BRITE path set(NS3_WITH_BRITE &quot;&quot; CACHE PATH &quot;Build with brite support&quot;) set(NS3_BRITE &quot;OFF&quot; CACHE INTERNAL &quot;ON if Brite is found in NS3_WITH_BRITE&quot;)]]></summary></entry><entry><title type="html">Blogging with LaTeX</title><link href="/2021/08/30/blogging-with-latex.html" rel="alternate" type="text/html" title="Blogging with LaTeX"/><published>2021-08-30T00:00:00+00:00</published><updated>2021-08-30T00:00:00+00:00</updated><id>/2021/08/30/blogging-with-latex</id><content type="html" xml:base="/2021/08/30/blogging-with-latex.html"><![CDATA[<h1 data-number="1" id="introduction"><span class="header-section-number">1</span> Introduction</h1> <p>I have always said to myself that I should start blogging to keep track of things (projects, side-projects, side-projects of side-projects), but I have a terrible memory and end up forgetting that.</p> <p>This time around I was thinking: - Hey, I should start blogging about something.</p> <p>But you know what, I really suck at managing this kind of stuff, Jekyll helps quite a lot but I still need to write stuff with markdown.</p> <p>I am not a huge fan of markdown either, patterns are weird, formatting tables is a mess, no equations, no bibtex (I also miss that when working with Word), and the list goes on.</p> <p>Every engineer knows the best way to solve problems is to solve the problem for themselves by overengineering a solution (this is meant as a joke... even though it sounds too accurate to be just that...).</p> <p>So, how can we make things better for ourselves and get some decent syntax for writing somewhat large texts?</p> <p>Here is a tip: it is in the title of this post.</p> <p>Yes, we can translate LaTeX into HTML, do some stripping, add a new Jekyll header and make things work nicely.</p> <p>But how do we do that? Pandoc (Section <a href="#Pandoc" data-reference-type="ref" data-reference="Pandoc">2</a>), Jekyll (Section <a href="#Jekyll" data-reference-type="ref" data-reference="Jekyll">3</a>) and some script glue (Section <a href="#glue" data-reference-type="ref" data-reference="glue">4</a>) written in the best programming language ever: Python.</p> <h1 data-number="2" id="Pandoc"><span class="header-section-number">2</span> Pandoc</h1> <p>Of course each one of us could write a LaTeX parser and do the conversion for ourselves (if we had unlimited time, big brains and were determined to do it).</p> <p>I do not know how about you, but I am of the kind that start writing programs right away just to realize how grand things will end up being.</p> <p>As usual when doing these things, you realize "ain’t nobody got time for that" <span class="citation" data-cites="AINT_GOT_TIME">(NobodyGotTimeForThis, n.d.)</span>.</p> <p>Started looking for alternatives. Found <a href="https://github.com/latex2html/latex2html">latex2html</a> and <a href="https://github.com/michal-h21/make4ht">make4ht</a>, which are pretty cool. Tried setting them up, but did not manage to make it work as I wanted. Started looking for more alternatives.</p> <p>Found a post in <a href="https://www.danwjoyce.com/data-blog/2018/2/20/latex-to-html-via-pandoc">Dr. Joyce’s blog</a>.</p> <p>Pandoc: An universal markdown translator, which can magically translate a subset of LaTeX+BibTex into HTML+MathJax (yay /o/, equations). It is not perfect, but it also does not need to be.</p> <p>While I did not quite manage to make equation numbering nor manage citations style/cross-references working correctly, <a href="https://github.com/tomduck/pandoc-xnos">pandoc-xnos</a> could make it work. I think it has to do with the glue in Section <a href="#glue" data-reference-type="ref" data-reference="glue">4</a>. Maybe I will try again in the future, but for now it is doing what I needed.</p> <p>The magic command ended up being</p> <pre><code>    pandoc --number-sections --mathjax -f latex -t html -s --bibliography=file.bib -o file.html file.tex</code></pre> <ul> <li><p><code>--number-sections</code> for numbered sections</p></li> <li><p><code>--mathjax</code> to process equations and render using javascript</p></li> <li><p><code>-f latex</code> to process from LaTeX</p></li> <li><p><code>-t html</code> to output in html</p></li> <li><p><code>-s</code> standalone html</p></li> <li><p><code>--bibliography=file.bib</code> to get references from the bib file</p></li> <li><p><code>-o file.html</code> to indicate the path to the output file</p></li> <li><p><code>file.tex</code> to indicate the input file</p></li> </ul> <h1 data-number="3" id="Jekyll"><span class="header-section-number">3</span> Jekyll</h1> <p>You probably have heard of <a href="https://jekyllrb.com/">Jekyll</a> at this point. It is a nice static site generator used by <a href="https://pages.github.com/">GitHub Pages</a>, which made a ton of people used to it by default. Jekyll does not support LaTeX documents, but it does accept HTML files as inputs.</p> <p>By placing the post header in the HTMLs, it will treat the HTML contents as the post content.</p> <pre><code>---
layout: post # could be a different layout
author: &quot;authorname&quot;
title: &quot;postname&quot;
---</code></pre> <h1 data-number="4" id="glue"><span class="header-section-number">4</span> Script glue</h1> <p>The final piece is the code glue written in Python. It just scans for <em>.tex</em> files, uses pandoc to convert them, strips out unnecessary HTML header/footer and include the post header expected by Jekyll containing the post author names, title and layout.</p> <p>If there is a <em>.bib</em> file along with the <em>.tex</em>, it is used as the bibliography source file.</p> <pre><code>def latex_to_html_via_pandoc(source_file, source_dir=&quot;latex_posts&quot;, target_dir=&quot;_posts&quot;):

    output_file = source_file.replace(&quot;.tex&quot;, &quot;.html&quot;).replace(source_dir, target_dir)

    # Latex to html
    command = &quot;&quot;&quot;pandoc --number-sections --mathjax -f latex -t html -s&quot;&quot;&quot;
    command = command.split(&quot; &quot;)

    # Load references from bib file if it exists
    bibliography = source_file.replace(&quot;.tex&quot;, &quot;.bib&quot;)
    if os.path.exists(bibliography):
        command.append(&quot;--bibliography=%s&quot; % bibliography)  # use external bib file or not

    command.append(&quot;-o&quot;)
    command.append(output_file)  # output path
    command.append(source_file)  # source path

    # Run pandoc
    try:
        subprocess.check_output(command)
    except Exception as e:
        raise Exception(&quot;Error during pandoc conversion of %s: %s&quot; % (source_file, e))

    # Open the html file and get only contents to let jekyll handle style, links, etc
    with open(output_file, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
        contents = f.read()

        # extract authors and title
        authors = re_match_html_author.findall(contents)
        author = authors[0]
        authors.pop(0)
        while len(authors) &gt; 0:
            author += &quot;, &quot; + authors[0]
            authors.pop(0)
        del authors
        title = re_match_html_title.findall(contents)[0]

        # Remove unnecessary header and trailer
        contents = contents.split(&quot;&lt;/header&gt;&quot;)[1]
        contents = contents.split(&quot;&lt;/body&gt;&quot;)[0]

    # Rewrite file with markdown header
    with open(output_file, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
        # Write markdown header
        f.write(&quot;&quot;&quot;---\nlayout: post\nauthor: &quot;%s&quot;\ntitle: &quot;%s&quot;\n---&quot;&quot;&quot; % (author, title))

        # Write stripped down html
        f.write(contents)</code></pre> <p>After that, you can manually run jekyll build command or jekyll serve (which starts the webserver).</p> <p>Or run <code>Latex2Markdown.py --serve</code> to run the LaTeX to HTML, Jekyll build and start service at once.</p> <p>Sources for this blog are available <a href="https://gabrielcarvfer.github.io/Latex2Markdown.py">here</a>.</p> <h1 data-number="5" id="conclusions"><span class="header-section-number">5</span> Conclusions</h1> <p>It works surprisingly well and I am pretty happy with the results.</p> <h1 class="unnumbered" data-number="" id="references">References</h1> <div id="refs" class="references hanging-indent" role="doc-bibliography"> <div id="ref-AINT_GOT_TIME"> <p>NobodyGotTimeForThis. n.d. “Ain’t Nobody Got Time for That (Original + Autotune).” <a href="https://youtu.be/waEC-8GFTP4">https://youtu.be/waEC-8GFTP4</a>.</p> </div> </div>]]></content><author><name>Gabriel</name></author><summary type="html"><![CDATA[1 Introduction I have always said to myself that I should start blogging to keep track of things (projects, side-projects, side-projects of side-projects), but I have a terrible memory and end up forgetting that. This time around I was thinking: - Hey, I should start blogging about something. But you know what, I really suck at managing this kind of stuff, Jekyll helps quite a lot but I still need to write stuff with markdown. I am not a huge fan of markdown either, patterns are weird, formatting tables is a mess, no equations, no bibtex (I also miss that when working with Word), and the list goes on. Every engineer knows the best way to solve problems is to solve the problem for themselves by overengineering a solution (this is meant as a joke... even though it sounds too accurate to be just that...). So, how can we make things better for ourselves and get some decent syntax for writing somewhat large texts? Here is a tip: it is in the title of this post. Yes, we can translate LaTeX into HTML, do some stripping, add a new Jekyll header and make things work nicely. But how do we do that? Pandoc (Section 2), Jekyll (Section 3) and some script glue (Section 4) written in the best programming language ever: Python. 2 Pandoc Of course each one of us could write a LaTeX parser and do the conversion for ourselves (if we had unlimited time, big brains and were determined to do it). I do not know how about you, but I am of the kind that start writing programs right away just to realize how grand things will end up being. As usual when doing these things, you realize "ain’t nobody got time for that" (NobodyGotTimeForThis, n.d.). Started looking for alternatives. Found latex2html and make4ht, which are pretty cool. Tried setting them up, but did not manage to make it work as I wanted. Started looking for more alternatives. Found a post in Dr. Joyce’s blog. Pandoc: An universal markdown translator, which can magically translate a subset of LaTeX+BibTex into HTML+MathJax (yay /o/, equations). It is not perfect, but it also does not need to be. While I did not quite manage to make equation numbering nor manage citations style/cross-references working correctly, pandoc-xnos could make it work. I think it has to do with the glue in Section 4. Maybe I will try again in the future, but for now it is doing what I needed. The magic command ended up being pandoc --number-sections --mathjax -f latex -t html -s --bibliography=file.bib -o file.html file.tex --number-sections for numbered sections --mathjax to process equations and render using javascript -f latex to process from LaTeX -t html to output in html -s standalone html --bibliography=file.bib to get references from the bib file -o file.html to indicate the path to the output file file.tex to indicate the input file 3 Jekyll You probably have heard of Jekyll at this point. It is a nice static site generator used by GitHub Pages, which made a ton of people used to it by default. Jekyll does not support LaTeX documents, but it does accept HTML files as inputs. By placing the post header in the HTMLs, it will treat the HTML contents as the post content. --- layout: post # could be a different layout author: &quot;authorname&quot; title: &quot;postname&quot; --- 4 Script glue The final piece is the code glue written in Python. It just scans for .tex files, uses pandoc to convert them, strips out unnecessary HTML header/footer and include the post header expected by Jekyll containing the post author names, title and layout. If there is a .bib file along with the .tex, it is used as the bibliography source file. def latex_to_html_via_pandoc(source_file, source_dir=&quot;latex_posts&quot;, target_dir=&quot;_posts&quot;):]]></summary></entry><entry><title type="html">Warp-speeding the captain of the Starfleet</title><link href="/metabrainz/2020/12/17/picard-the-captain-of-the-starfleet.html" rel="alternate" type="text/html" title="Warp-speeding the captain of the Starfleet"/><published>2020-12-17T00:00:00+00:00</published><updated>2020-12-17T00:00:00+00:00</updated><id>/metabrainz/2020/12/17/picard-the-captain-of-the-starfleet</id><content type="html" xml:base="/metabrainz/2020/12/17/picard-the-captain-of-the-starfleet.html"><![CDATA[<p>Have you ever heard of <a href="https://picard.musicbrainz.org/">MusicBrainz Picard</a>? It is an amazing piece of software that makes tagging music files a breeze. Not only that, but it can also be used to fingerprint your music files, identify the recording and pull up metadata from the MusicBrainz servers.</p> <p>I got some spare time earlier this year and decided to tag my long-abandoned music library. Started by downloading the latest version of Picard, then installed it. After that I executed the binary and finally clicked to scan my library folder.</p> <p>And then I waited… And waited… And waited… Jesus, what is this thing doing? It was barely touching the disk drive, but the single thread was pinned at 100% load. It clearly had a bottleneck somewhere and I had to find it. Started looking into the code that performed the load and the profilers quickly indicated troublesome spots, which I thought could be spread into multiple processes to speed things up. Spent some time trying to make it work and failed miserably due to circular dependencies between the logic components and the GUI.</p> <p>Then I tried looking in the forums because I must be doing something wrong. I have found a few people complaining about performance. After that, I dug their issue tracker and hit gold. <a href="https://tickets.metabrainz.org/browse/PICARD-975">Issue 975 - Reimplement the threading code in Picard</a>.</p> <p>One of Picard’s contributors, Sophist gave me a few hints <a href="https://tickets.metabrainz.org/browse/PICARD-975?focusedCommentId=53339&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-53339">where to look at</a>. After assuming the bottleneck was CPU-bound, I’ve proposed a <a href="https://tickets.metabrainz.org/browse/PICARD-975?focusedCommentId=53340&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-53340">multi-process architecture</a>. It ended up being a dumb way to deal with the issue, but we will come to that later.</p> <p><img src="/assets/img/2020-12-17-picard-the-captain-of-the-starfleet/picard_multiprocess.png" alt="drawing" width="100%"/></p> <p>We then discussed a bit and I started looking into the areas suggested by Sophist. I decided that batch processing would probably speed things up, as instructions/data would be cached. I was partially correct, and managed to reduce load times from <a href="https://tickets.metabrainz.org/browse/PICARD-975?focusedCommentId=53349&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-53349">20 minutes down to 8 minutes</a>. It was huge. Got pretty excited with the results and decided to properly profile things.</p> <p>And then we hit the first major breakthrough:</p> <h4 id="1---when-the-slow-path-is-too-slow-having-a-fast-path-is-a-good-idea">1 - When the slow path is too slow, having a fast path is a good idea</h4> <p>The cProfiler indicated that most of the remaining time during load was due to trying to guess the file format based on file headers instead of using the file extension. It so happens that most of the time the files extensions are correct, and we could skip trying to identify the file. If the load failed, we could fallback to trying to guess the file format. That alone reduced load times even further, from 20 minutes down to 3.5 minutes. It became my first <a href="https://github.com/metabrainz/picard/pull/1529">pull request</a> for Picard. Pretty exciting.</p> <hr/> <p>After that, I tried a bunch more stuff to make things faster, but nothing seemed to change. Then I started looking at the IO performance. Mainstream Picard barely reached 3MBps while reading the files. The batch version was much faster, reaching up to <a href="https://tickets.metabrainz.org/browse/PICARD-975?focusedCommentId=53354&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-53354">39MBps</a>, but it also prevented the UI from updating, causing freezes.</p> <p>There was definitely something messed up, and thread starvation seemed to make sense. It so happens that the directory scanning thread, that fed file loading threads, was executed concurrently instead of running at once and then dispatching the work. That made Picard randomly access the disk while trying to load songs at the same time it scanned directories. A.k.a. the worst-case scenario for HDDs.</p> <p>By scanning first and processing later, we drastically reduced thread wait times.</p> <p><img src="/assets/img/2020-12-17-picard-the-captain-of-the-starfleet/picard_threading_profiling.png" alt="drawing" width="100%"/></p> <p>And then we hit the second major breakthrough:</p> <h4 id="2---poor-loading-threads-were-starving-to-death-while-scanning-thread-was-trying-to-keep-up">2 - Poor loading threads were starving to death while scanning thread was trying to keep up</h4> <p>Improving the scanning thread and letting it completely scan the directories before dispatching work for other threads was essential to boost performance. This change alone reduced loading times of a different library from 32 minutes down to 13 minutes, as reported in my second <a href="https://github.com/metabrainz/picard/pull/1531">pull request</a>.</p> <hr/> <p>We ended up moving discussion from their issue tracker to that previous pull request thread. The IO issues seemed to be solved, but now it was the UI that could not keep up with the file loading, causing freezes and stutters. This is where things turned out way more difficult to profile than I initially expected.</p> <p>Tried some really complicated stuff, like <a href="https://github.com/metabrainz/picard/pull/1531#issuecomment-625004549">caching metadata to skip reloading files</a> and part of the <a href="https://github.com/metabrainz/picard/pull/1531#issuecomment-626184738">multi-process architecture</a>.</p> <p>Profilers pointed at different things all the time: first <a href="https://github.com/metabrainz/picard/pull/1531#issuecomment-623181143">updating the file items</a>, then <a href="https://github.com/metabrainz/picard/pull/1531#issuecomment-626377501">the file loading finalization</a>, then <a href="https://github.com/metabrainz/picard/pull/1531#issuecomment-629534654">the sorting algorithm</a>, and finally <a href="https://github.com/metabrainz/picard/pull/1531#issuecomment-629534654">the sorting of the panels</a>.</p> <p>Surprisingly, temporarily disabling the sorting of the panels was super effective, and our third breakthrough.</p> <h4 id="3---sorting-over-and-over-an-ever-increasing-list-is-insane-and-ui-draws-are-super-expensive">3 - Sorting over and over an ever-increasing list is insane and UI draws are super expensive</h4> <p>We disabled the sorting during operations that involve moving/adding/removing large amounts of files (e.g. loading/clustering). This reduced drawing times of the items in the panel from <a href="https://github.com/metabrainz/picard/pull/1531#issuecomment-629534654">30 minutes down to 1 minute</a>.</p> <hr/> <p>It was at this point I realized the previous changes were way too much and we could achieve basically the same great results with fewer changes. Remember when I said the multi-process solution was dumb? This is the point I realized it was the case.</p> <p>Putting the previous 2 breakthroughs into a <a href="https://github.com/metabrainz/picard/pull/1543">single PR</a>, we manage to reduce loading times of a third library from <strong>50m down to 6m</strong>. At the same time, clustering times fell from <strong>5h17min down to 3 minutes</strong>.</p> <p>But I was not done just yet. I could feel the profilers were messing with me. How could I have changed a ton of stuff, and instead of processing times falling they would just move around to different functions? Quite puzzling.</p> <p>The answer was the profilers just could not measure the problem because it was not in python, but outside python. What do I mean by that? The C libraries that took the GIL and blocked python were the culprits. I decided to move some of the modules to cython to track exactly where the calls were made and voilà. Two missing Qt size hints were <a href="https://github.com/metabrainz/picard/pull/1555#issuecomment-644475419">two large contributors to the UI slowdown</a>.</p> <p>Our fourth breakthrough:</p> <h4 id="4---qt-documentation-is-poor-and-profiling-mixed-language-software-is-nightmarish">4 - Qt documentation is poor, and profiling mixed language software is nightmarish</h4> <p>Yes, two Qt size hints were enough to improve the UI responsiveness by a huge margin. Those were two one-liners but reduced the CPU samples from 40% down to 10% of total samples. This means Picard can actually do useful stuff instead of guessing sizes during repaints of the UI.</p> <p><em>Without hints</em> = 40% of CPU samples <img src="/assets/img/2020-12-17-picard-the-captain-of-the-starfleet/picard_without_hint.png" alt="drawing" width="100%"/></p> <p><em>With hints</em> = 10% of CPU samples <img src="/assets/img/2020-12-17-picard-the-captain-of-the-starfleet/picard_with_hint.png" alt="drawing" width="100%"/></p> <p>Side effects? <a href="https://github.com/metabrainz/picard/pull/1555#issuecomment-644860437">150x speedup</a> on moving files during clustering with expanded clusters (worst case scenario) + sorting disabled. People used to recommend collapsing the clusters to speed things up, but it got solved by two one-line hints. Completely crazy.</p> <hr/> <p>With this, I was pretty much done, but the UI still had some freezing/hiccup issues, especially in Windows. After further profiling, I have noticed the worker threads had a lot of contention while checking configuration variables. They spent a ton of time waiting for each other due to locks. To prevent them from locking, we only locked the reader threads if the cached setting was marked as stale/dirty or when a thread needed the lock to update the setting. This was done in this <a href="https://github.com/metabrainz/picard/commit/18c4e93cf1073e21857e4e39d311eb28d9d8f76f">commit</a>.</p> <p>Other area of Picard that drastically slowed things down was selecting tracks. They updated the metadata box, which can be awfully expensive depending on how many items are selected. I tried to mitigate the issue <a href="https://github.com/metabrainz/picard/commit/6ef6679f65e68796cfdf8be2d63307f5b1400305">here</a>, by preventing metadata box updates if the selection didn’t change. It seemed very logical to me (as I am trying to prevent reprocessing everything), but it ended up causing issues to other users. Tried a few more changes later, but it still ended up being changed as users reported other issues… My bad.</p> <h3 id="end-results">End results</h3> <p>As reported in ticket <a href="https://tickets.metabrainz.org/browse/PICARD-1844">PICARD-1844 - Further improve loading and clustering performance</a>, loading times fell from 50m down to 5m30s (~9x speedup), and clustering times fell from 5h to 2m (~150x speedup) in my library with 19k files. UI responsiveness is way harder to measure, so no hard measures on that, but you can try it by yourself. Touching mere 700 lines of code instead of rearchitecting the entire thing as I initially thought would be necessary… Changes were released in Picard 2.4.</p> <p>It was pretty fun and the results were great. Laurent (Zas), Philipp (outsidecontext) and Sophist (I don’t know his real name) were awesome and helped a lot.</p>]]></content><author><name></name></author><category term="MetaBrainz"/><summary type="html"><![CDATA[Have you ever heard of MusicBrainz Picard? It is an amazing piece of software that makes tagging music files a breeze. Not only that, but it can also be used to fingerprint your music files, identify the recording and pull up metadata from the MusicBrainz servers.]]></summary></entry></feed>