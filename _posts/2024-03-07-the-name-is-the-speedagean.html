---
layout: post
author: "Gabriel"
title: "The name... is speedagean: Improving performance of LTE and NR in ns-3"
---
<h1 data-number="1" id="ever-growing-complexity-and-performance-implications"><span class="header-section-number">1</span> Ever growing complexity and performance implications</h1>
<p>I haven’t blogged in a while. Didn’t have the time to do it. During this time, I left the <a href="https://cic.unb.br/">Universidade de Brasília</a> and joined <a href="https://www.cttc.cat/">CTTC</a>.</p>
<p>One of CTTC clients was concerned with the growing complexity and simulation runtime, which drastically limit the scalability of simulations, unless you want to wait a few days for a few seconds worth of a large simulation scenario to finish.</p>
<p>I have been working on performance optimization since 2010, during my undergrad thesis/final work, so it is just natural I needed to profile the simulations and provide some improvements.</p>
<p>The latest feature CTTC has been working on is MIMO, which allows for the transmission of multiple spatial data streams to increase throughput and reduce latency. This is done by using a wack ton of antennas in an array, that are controlled in groups called ’antenna ports’.</p>
<p>Note: I am no specialist in MIMO, so feel free to correct me.</p>
<p>The problem with these many antennas and ports, is that we used to have a single channel calculation per UE-eNB/gNB pair transmission per subband (resource block), which was already split into beam clusters. So we already had <span class="math inline">\(O(n^3)\)</span> complexity, implemented with 3 nested for loops. Now, we need to do these same calculation, but FOR EACH AND EVERY SENDER/RECEIVER PORT PAIR COMBINATION. This means our complexity grew to <span class="math inline">\(O(n^5)\)</span>.</p>
<p>Note: it is not actually <span class="math inline">\(O(n^5)\)</span>, but <span class="math inline">\(O(nRb\times nReceivers \times nAntennaTx \times nAntennaRx \times nCluster)\)</span>, but I prefer the compact version for simplicity.</p>
<p>And it gets worse, because this is just one of the computations that needs to be done. MIMO is typically implemented using a codebook to search for the appropriate precoding matrix, which the UE brute-forces a search to find the indices of the codebook known as <span class="math inline">\(i_{1}\)</span> and <span class="math inline">\(i_{2}\)</span> for wide-band and sub-band. This process consumes a lot of power in hardware, which sucks for battery, but at least it is fast, since the functional units that make the calculations can be simply copy-pasted to test and compare multiple combinations quickly.</p>
<p>You don’t have that luxury when working with software. So you got to use hardware efficiently.</p>
<p>I’m not going to enter into details on how to profile anything, because <a href="https://www.nsnam.org/docs/manual/html/profiling.html">I’ve have already written an entire section in the ns-3 manual for that</a>.</p>
<p>Just let’s review the basics on graphical analysis (this isn’t real science, but whatever...).</p>
<p>Below we have Figure <a href="#fig:profiling-basics" data-reference-type="ref" data-reference="fig:profiling-basics">1</a>, showing how the profiling of a program looks like. In the vertical axis, we have the program stack, which gets sampled by the profiler every few milliseconds or whatever frequency you configure it to sample.</p>
<p>In the horizontal axis you have 100% of total CPU time. This is important, because optimizations are not going to reduce the size of these bars, just the size of the bar of the piece of code that was optimized. The rest of the code will have their bars widened, to show they are now more significant.</p>
<figure>
<img src="/assets/img/2024-03-07-the-name-is-the-speedagean/profiling-basics.png" id="fig:profiling-basics" style="width:100.0%" alt="" /><figcaption>Performance profiling of cttc-nr-mimo-demo with MIMO feedback, of release nr-3.0.y</figcaption>
</figure>
<p>With this introduction, let’s proceed to some profiling results and optimizations for their primary bottlenecks.</p>
<h1 data-number="2" id="default-cttc-nr-mimo-demo-settings-with-mimo-feedback"><span class="header-section-number">2</span> Default cttc-nr-mimo-demo settings with MIMO feedback</h1>
<p>This scenario is pretty vanilla, with small number of antennas, antenna ports, just a single UE and gNB, in a static scenario. First, let’s take a look at the initial profiling results. This was taken with nr-3.0 and ns-3.41 releases. Following profiles are cumulative, getting speedups from the previous optimizations.</p>
<h2 data-number="2.1" id="delay-computation-in-threegppspectrumpropagationlossmodel"><span class="header-section-number">2.1</span> Delay computation in ThreeGppSpectrumPropagationLossModel</h2>
<p>You can see in Figure <a href="#fig:cache-sincos-before" data-reference-type="ref" data-reference="fig:cache-sincos-before">2</a> that a huge fraction of the total simulation time was spent on ThreeGppSpectrumPropagationLossModel::CalcBeamformingGain, and most of it is calculating sincos.</p>
<figure>
<img src="/assets/img/2024-03-07-the-name-is-the-speedagean/cache-sincos-before.png" id="fig:cache-sincos-before" style="width:100.0%" alt="" /><figcaption>Before optimization</figcaption>
</figure>
<p>This sincos means we are computing both a sin and a cosine of the same angle together, which is more efficient than separately.</p>
<p>The respective piece of code that calls these many sincos is the following loop:</p>
<pre><code> while (vit != inPsd-&gt;ValuesEnd())
    {
        if ((*vit) != 0.00)
        {
            double fsb = (*sbit).fc; // center frequency of the sub-band
            for (auto rxPortIdx = 0; rxPortIdx &lt; numRxPorts; rxPortIdx++)
            {
                for (auto txPortIdx = 0; txPortIdx &lt; numTxPorts; txPortIdx++)
                {
                    std::complex&lt;double&gt; subsbandGain(0.0, 0.0);

                    for (size_t cIndex = 0; cIndex &lt; numCluster; cIndex++)
                    {
                        double delay = -2 * M_PI * fsb * (channelParams-&gt;m_delay[cIndex]);
                        subsbandGain += directionalLongTerm(rxPortIdx, txPortIdx, cIndex) *
                                        doppler[cIndex] *
                                        std::complex&lt;double&gt;(cos(delay), sin(delay)); // &lt;&lt;&lt;&lt; HERE
                    }
                    // Multiply with the square root of the input PSD so that the norm (absolute
                    // value squared) of chanSpct will be the output PSD
                    chanSpct-&gt;Elem(rxPortIdx, txPortIdx, iRb) = sqrt(*vit) * subsbandGain;
                }
            }
        }
        vit++;
        sbit++;
        iRb++;
    }</code></pre>
<p>To save up in so many sincos, I’ve tried a bunch of different techniques: memoization, lookup tables, cordic, fast sincos implementations, but none gave great results.</p>
<p>And the reason is simple: this is the <span class="math inline">\(O(n^5)\)</span> loop I mentioned before. You can’t see the loop above it, so it is technically <span class="math inline">\(O(n^4)\)</span>, but this our archnemesis.</p>
<p>If we want to scale, we need to get this sincos out of here in some way. After carefully inspecting where exactly we get the <em>m_delay</em>, I noticed these values are not always updated, so there is no need for us to recompute them every single time. Also, they do not even depend in the number of ports, so it is criminally expensive to put them inside of these loops. You would hope the compiler to optimize that, but you don’t get these kind of optimization while debugging.</p>
<p>What I ended up doing was to cache these delay calculation until channel conditions change. These were merged in <a href="https://gitlab.com/nsnam/ns-3-dev/-/merge_requests/1427">ns-3-dev MR1427</a>.</p>
<p>The caching section is the following:</p>
<pre><code>// Precompute the delay until numRb, numCluster or RB width changes
    // Whenever the channelParams is updated, the number of numRbs, numClusters
    // and RB width (12*SCS) are reset, ensuring these values are updated too
    double rbWidth = inPsd-&gt;ConstBandsBegin()-&gt;fh - inPsd-&gt;ConstBandsBegin()-&gt;fl;

    if (channelParams-&gt;m_cachedDelaySincos.GetNumRows() != numRb ||
        channelParams-&gt;m_cachedDelaySincos.GetNumCols() != numCluster ||
        channelParams-&gt;m_cachedRbWidth != rbWidth)
    {
        channelParams-&gt;m_cachedRbWidth = rbWidth;
        channelParams-&gt;m_cachedDelaySincos = ComplexMatrixArray(numRb, numCluster);
        auto sbit = inPsd-&gt;ConstBandsBegin(); // band iterator
        for (unsigned i = 0; i &lt; numRb; i++)
        {
            double fsb = (*sbit).fc; // center frequency of the sub-band
            for (std::size_t cIndex = 0; cIndex &lt; numCluster; cIndex++)
            {
                double delay = -2 * M_PI * fsb * (channelParams-&gt;m_delay[cIndex]);
                channelParams-&gt;m_cachedDelaySincos(i, cIndex) =
                    std::complex&lt;double&gt;(cos(delay), sin(delay));
            }
            sbit++;
        }
    }</code></pre>
<p>And now we just read the values from <em>channelParamsm_cachedDelaySincos</em>. in the loop shown before.</p>
<p>The results are perplexing, in my humble opinion: 1.36x speedup in total simulation time, and 11x speedup in that specific function, as shown in Figure <a href="#fig:cache-sincos-after" data-reference-type="ref" data-reference="fig:cache-sincos-after">3</a>.</p>
<figure>
<img src="/assets/img/2024-03-07-the-name-is-the-speedagean/cache-sincos-after.png" id="fig:cache-sincos-after" style="width:100.0%" alt="" /><figcaption>After optimization</figcaption>
</figure>
<p>We are in for a great start.</p>
<h2 data-number="2.2" id="exponential-sinr-in-nreesmerrormodel"><span class="header-section-number">2.2</span> Exponential SINR in NrEesmErrorModel</h2>
<p>If you look at the profiling results from the previous section, you are going to notice the largest column is now an <em>exp</em> function call.</p>
<p>This <em>exp</em> is called in the following function:</p>
<pre><code>double
NrEesmErrorModel::SinrExp(const SpectrumValue&amp; sinr, const std::vector&lt;int&gt;&amp; map, uint8_t mcs) const
{
    // it returns sum_n (exp (-SINR/beta))
    NS_LOG_FUNCTION(sinr &lt;&lt; &amp;map &lt;&lt; (uint8_t)mcs);
    NS_ABORT_MSG_IF(map.empty(),
                    &quot; Error: number of allocated RBs cannot be 0 - EESM method - SinrEff function&quot;);

    double SINRexp = 0.0;
    double SINRsum = 0.0;
    double beta = GetBetaTable()-&gt;at(mcs);
    for (int i : map)
    {
        double sinrLin = sinr[i];
        SINRexp = exp(-sinrLin / beta);
        SINRsum += SINRexp;
    }
    return SINRsum;
}</code></pre>
<p>There isn’t anything crazy about this function, other than the slow <em>exp</em> math function. In the Figure <a href="#fig:sinr-exp-before" data-reference-type="ref" data-reference="fig:sinr-exp-before">4</a> below we can see the profiling results before the proposed optimizations.</p>
<figure>
<img src="/assets/img/2024-03-07-the-name-is-the-speedagean/sinr-exp-before.png" id="fig:sinr-exp-before" style="width:100.0%" alt="" /><figcaption>Before optimization</figcaption>
</figure>
<p>But it turns out there are plenty of brilliant mathematicians working tirelessly to produce fast approximations for these math primitives. I found one of these approximations in "Simple multiple precision algorithms for exponential functions", by Leonid Moroz, Volodymyr Samotyy, Zbigniew Kokosiński and Paweł Gepner, published in the <a href="https://ieeexplore.ieee.org/document/9810030">IEEE Signal Processing Magazine 39(4):130 - 137 (2022)</a>.</p>
<p>Their solution works brilliantly and is a lot faster than the default glibc exponential. I am very grateful that they accepted to license their code under the very permissive MIT license.</p>
<p>Their implementation for floats look like this:</p>
<pre><code>inline float
exp21f(float x)
{
    int z = x * 0x00b8aa3b + 0x3f800000;

    union {
        int i;
        float f;
    } zii;

    zii.i = z &amp; 0x7f800000;
    int zif = z &amp; 0x007fffff;
    float d1 = 0.40196114e-7f;
    float d2 = 0xf94ee7 + zif;
    float d3 = 0x560e + zif;
    d2 = d1 * d2;
    zif = d2 * d3;
    zii.i |= zif;
    float y = zii.f;
    return y;
}</code></pre>
<p>After replacing the original code with this one, plus a few additional guards, we get the following results in Figure <a href="#fig:sinr-exp-after" data-reference-type="ref" data-reference="fig:sinr-exp-after">5</a>:</p>
<figure>
<img src="/assets/img/2024-03-07-the-name-is-the-speedagean/sinr-exp-after.png" id="fig:sinr-exp-after" style="width:100.0%" alt="" /><figcaption>After optimization</figcaption>
</figure>
<p>Fantastic 1.32x total simulation speedup, and 4.5x speedup in that particular function.</p>
<p>I was very concerned about this patch though, because it is a key value used to determine the Block-Level Error Rate (BLER) and eventually the Modulation and Coding Scheme (MCS), which can make or break the system. In fact, when I tested a different fast implementation, simulations broke due to the loss of precision, but this implementation provides enough precision for our use case. An example of that is shown in the Figure <a href="#fig:sinr-exp-prec" data-reference-type="ref" data-reference="fig:sinr-exp-prec">6</a> below:</p>
<figure>
<img src="/assets/img/2024-03-07-the-name-is-the-speedagean/sinr-exp-prec.png" id="fig:sinr-exp-prec" style="width:100.0%" alt="" /><figcaption>Comparison between precision of custom Exp() function</figcaption>
</figure>
<p>Which means we are good in most cases, but we are still testing it, in case we missed some important case in our automated test suites. This patch is currently available as the <a href="https://gitlab.com/cttc-lena/nr/-/merge_requests/111">nr MR111</a>.</p>
<p>So far, so good. To the next bottleneck.</p>
<h2 data-number="2.3" id="keep-traces-file-open-throughout-the-simulation"><span class="header-section-number">2.3</span> Keep traces file open throughout the simulation</h2>
<p>This one isn’t anything new. I’ve repeated that time and time again. In fact I fixed the same issue in LTE, in <a href="https://gitlab.com/nsnam/ns-3-dev/-/merge_requests/814">ns-3 MR814</a>.</p>
<p>Opening and closing files during the simulation can be incredibly slow in slow filesystems (e.g. HDD, network mounts such as the one used by Windows WSL, Windows due to garbage that is Windows defender).</p>
<p>The code for this was already merged upstream in <a href="https://gitlab.com/cttc-lena/nr/-/merge_requests/113">nr MR113</a>.</p>
<p>Below are the Before <a href="#fig:keep-traces-open-before" data-reference-type="ref" data-reference="fig:keep-traces-open-before">7</a>, and After <a href="#fig:keep-traces-open-after" data-reference-type="ref" data-reference="fig:keep-traces-open-after">8</a> figures, showing the 1.16x total simulation speedup, or 18x speedup for that particular function.</p>
<figure>
<img src="/assets/img/2024-03-07-the-name-is-the-speedagean/keep-traces-open-before.png" id="fig:keep-traces-open-before" style="width:100.0%" alt="" /><figcaption>Before optimization</figcaption>
</figure>
<figure>
<img src="/assets/img/2024-03-07-the-name-is-the-speedagean/keep-traces-open-after.png" id="fig:keep-traces-open-after" style="width:100.0%" alt="" /><figcaption>After optimization</figcaption>
</figure>
<h2 data-number="2.4" id="prevent-unnecessary-copy"><span class="header-section-number">2.4</span> Prevent unnecessary copy</h2>
<p>C++ kind of sucks in being explicit about deep copies. In fact, it is very easy to deep copy a huge structure and pay a hefty price for a single missing <em>&amp;</em> character.</p>
<p>And that happened in the following code of nr:</p>
<pre><code>double
NrEesmErrorModel::MappingSinrBler(double sinr, uint8_t mcs, uint32_t cbSizeBit)
{
...
    const auto cbMap = GetSimulatedBlerFromSINR()-&gt;at(bg_type).at(mcs); // &lt;&lt;&lt;&lt; HERE
    auto cbIt = cbMap.upper_bound(cbSizeBit);
...</code></pre>
<p>Adding the missing <em>&amp;</em> resulted in a 1.15x total simulation speedup, or 18x speedup in that particular function.</p>
<figure>
<img src="/assets/img/2024-03-07-the-name-is-the-speedagean/prevent-copy-before.png" id="fig:prevent-copy-before" style="width:100.0%" alt="" /><figcaption>Before optimization</figcaption>
</figure>
<figure>
<img src="/assets/img/2024-03-07-the-name-is-the-speedagean/prevent-copy-after.png" id="fig:prevent-copy-after" style="width:100.0%" alt="" /><figcaption>After optimization</figcaption>
</figure>
<h2 data-number="2.5" id="using-the-appropriate-container-type"><span class="header-section-number">2.5</span> Using the appropriate container type</h2>
<p>C++ offers a ton of containers that behave very similar but are implemented very differently, which means they have different asymptotic performance characteristics. For example eliminating the front element in a std::vector&lt;&gt; has a complexity of <span class="math inline">\(O(n)\)</span>, while eliminating the front element of a std::deque&lt;&gt; has a complexity of <span class="math inline">\(O(1)\)</span>.</p>
<p>Which basically means that as long as <span class="math inline">\(n\)</span> is small, they both perform similarly, however that was not the case in the <em>m_txBuffer</em> of LteRlcUm, currently shared by the NR module.</p>
<p>This is what the code looked like:</p>
<pre><code>void
LteRlcUm::DoNotifyTxOpportunity(LteMacSapUser::TxOpportunityParameters txOpParams)
{
    ...
    while (firstSegment &amp;&amp; (firstSegment-&gt;GetSize() &gt; 0) &amp;&amp; (nextSegmentSize &gt; 0))
    {
        NS_LOG_LOGIC(&quot;WHILE ( firstSegment &amp;&amp; firstSegment-&gt;GetSize &gt; 0 &amp;&amp; nextSegmentSize &gt; 0 )&quot;);
        NS_LOG_LOGIC(&quot;    firstSegment size = &quot; &lt;&lt; firstSegment-&gt;GetSize());
        NS_LOG_LOGIC(&quot;    nextSegmentSize   = &quot; &lt;&lt; nextSegmentSize);
        if ((firstSegment-&gt;GetSize() &gt; nextSegmentSize) ||
            // Segment larger than 2047 octets can only be mapped to the end of the Data field
            (firstSegment-&gt;GetSize() &gt; 2047))
        {
           ...
        }
        else // (firstSegment-&gt;GetSize () &lt; m_nextSegmentSize) &amp;&amp; (m_txBuffer.size () &gt; 0)
        {
            ...
            m_txBuffer.erase(m_txBuffer.begin()); // &lt;&lt;&lt;&lt; HERE
            NS_LOG_LOGIC(&quot;        txBufferSize = &quot; &lt;&lt; m_txBufferSize);
        }
    }
...
}</code></pre>
<p>By replacing the std::vector&lt;&gt; with a std::deque&lt;&gt; we got a 1.1x total simulation speedup, or 4.8x speedup in that particular function.</p>
<figure>
<img src="/assets/img/2024-03-07-the-name-is-the-speedagean/use-deque-before.png" id="fig:use-deque-before" style="width:100.0%" alt="" /><figcaption>Before optimization</figcaption>
</figure>
<figure>
<img src="/assets/img/2024-03-07-the-name-is-the-speedagean/use-deque-after.png" id="fig:use-deque-after" style="width:100.0%" alt="" /><figcaption>After optimization</figcaption>
</figure>
<h2 data-number="2.6" id="and-there-is-more..."><span class="header-section-number">2.6</span> And there is more...</h2>
<p>Still working on other smaller optimizations, but nothing will ever be as significant as these ones.</p>
<p>Unless we go to more challenging scenarios, which we did.</p>
<h1 data-number="3" id="absurd-mimo-settings-in-cttc-nr-mimo-demo"><span class="header-section-number">3</span> Absurd MIMO settings in cttc-nr-mimo-demo</h1>
<p>In this section, we will be using the following configuration:</p>
<ul>
<li><p>gNB with 2x32 antenna array, 2x8 ports, dual polarized (total of 32 ports)</p></li>
<li><p>UE with 2x8 antenna array, 2x8 ports, dual polarized (total of 32 ports)</p></li>
</ul>
<p>Our client and my boss said these settings are not representative, but like I said, I am not specialist in MIMO, so these don’t have to be representative at all for the work I am doing. Since they are so extreme, they help to point out where the code scales badly in the new MIMO feature. And in the 2024 MWC, there were apparently radios with 256 ports for 6G... So we better get this scaling well to be future-proof.</p>
<p>These number of ports mean that our <span class="math inline">\(O(n^5)\)</span> is already in the range of <span class="math inline">\(O(1024*n^3)\)</span>. So whatever we do, we must do as cheaply as possible inside that loop.</p>
<p>After profiling the code again, we saw the unthinkable: the MIMO codebook search took 82% of the total simulation time.</p>
<p>More on that in the next section.</p>
<h2 data-number="3.1" id="exhaustive-search-can-be-optimal-but-it-is-very-exhausting"><span class="header-section-number">3.1</span> Exhaustive search can be optimal, but it is very exhausting</h2>
<p>In the case of MIMO codebook search, the exhaustive PMI search implemented by NrPmSearchFull is extremely slow because the search space is very big.</p>
<p>I spent two weeks trying to understand how on earth MIMO works, and I got some very superficial understanding, that was enough to identify what we could do. My boss then said we could extract the rank and search the wideband <span class="math inline">\(i_1\)</span> index from the channel average, and I did it. With these two, we can skip the vast majority of the search space, and focus on finding subband <span class="math inline">\(i_2\)</span> indices.</p>
<p>The full search complexity is <span class="math inline">\(O(nRanks*nI1s*nSubBands*nI2s)\)</span>, which we could say is close to <span class="math inline">\(O(n^4)\)</span> in search space shape. The nI1s and nSubBands can be extremely large numbers.</p>
<p>After reordering operations, we get a fast search with <span class="math inline">\(O(nSubBands+1+nI1s*nI2s+nSubBands*nI2s)\)</span>, which is close to <span class="math inline">\(O(n^2)\)</span> in search space shape.</p>
<p>The results are as amazing as this sounds, as can be seen in the before <a href="#fig:search-fast-before" data-reference-type="ref" data-reference="fig:search-fast-before">13</a> and after <a href="#fig:search-fast-after" data-reference-type="ref" data-reference="fig:search-fast-after">14</a> Figures.</p>
<p>5x total simulation speedup, and 25x faster CreateCqiFeedbackMimo.</p>
<figure>
<img src="/assets/img/2024-03-07-the-name-is-the-speedagean/search-fast-before.png" id="fig:search-fast-before" style="width:100.0%" alt="" /><figcaption>Before optimization</figcaption>
</figure>
<figure>
<img src="/assets/img/2024-03-07-the-name-is-the-speedagean/search-fast-after.png" id="fig:search-fast-after" style="width:100.0%" alt="" /><figcaption>After optimization</figcaption>
</figure>
<p>And no, we are not done yet.</p>
<h2 data-number="3.2" id="operating-on-complex-isnt-as-cheap-as-people-would-expect"><span class="header-section-number">3.2</span> Operating on complex isn’t as cheap as people would expect</h2>
<p>As it can be seen in the previous figures, GenSpectrumChannelMatrix is back from the dead to torment us. It is the function contains the <span class="math inline">\(O(n^5)\)</span> archnemesis loops I mentioned at the beginning.</p>
<p>We took the sincos out of it, but the multiplications inside it are still expensive. Especially since we are operating on top of complex numbers, where each product is actually composed of 4 double multiplications plus 2 adds.</p>
<p>Let us have some flashbacks by looking at the loops again.</p>
<pre><code>while (vit != inPsd-&gt;ValuesEnd())
    {
        if ((*vit) != 0.00)
        {
            for (auto rxPortIdx = 0; rxPortIdx &lt; numRxPorts; rxPortIdx++)
            {
                for (auto txPortIdx = 0; txPortIdx &lt; numTxPorts; txPortIdx++)
                {
                    std::complex&lt;double&gt; subsbandGain(0.0, 0.0);
                    for (size_t cIndex = 0; cIndex &lt; numCluster; cIndex++)
                    {
                        subsbandGain += directionalLongTerm(rxPortIdx, txPortIdx, cIndex) *
                                        doppler[cIndex] *
                                        channelParams-&gt;m_cachedDelaySincos(iRb, cIndex);
                    }
                    // Multiply with the square root of the input PSD so that the norm (absolute
                    // value squared) of chanSpct will be the output PSD
                    chanSpct-&gt;Elem(rxPortIdx, txPortIdx, iRb) = sqrt(*vit) * subsbandGain;
                }
            }
        }
    }</code></pre>
<p>Notice that <em>m_cachedDelaySincos</em> and <em>doppler</em> depend on cIndex, but are completely independent on the number of ports.</p>
<p>But we are doing that multiplication anyways...</p>
<p>We can actually move all of this outside the loops, and create a temporary array with these values.</p>
<pre><code>    // Compute the product between the doppler and the delay sincos
    auto delaySincosCopy = channelParams-&gt;m_cachedDelaySincos;
    for (size_t iRb = 0; iRb &lt; inPsd-&gt;GetValuesN(); iRb++)
    {
        for (std::size_t cIndex = 0; cIndex &lt; numCluster; cIndex++)
        {
            delaySincosCopy(iRb, cIndex) *= doppler[cIndex];
        }
    }
    ...
    while (vit != inPsd-&gt;ValuesEnd())
    {
        if ((*vit) != 0.00)
        {
            auto sqrtVit = sqrt(*vit);
            for (auto rxPortIdx = 0; rxPortIdx &lt; numRxPorts; rxPortIdx++)
            {
                for (auto txPortIdx = 0; txPortIdx &lt; numTxPorts; txPortIdx++)
                {
                    std::complex&lt;double&gt; subsbandGain(0.0, 0.0);
                    for (size_t cIndex = 0; cIndex &lt; numCluster; cIndex++)
                    {
                        subsbandGain += directionalLongTerm(rxPortIdx, txPortIdx, cIndex) *
                                        delaySincosCopy(iRb, cIndex); // &lt;&lt;&lt;&lt; HERE
                    }
                    // Multiply with the square root of the input PSD so that the norm (absolute
                    // value squared) of chanSpct will be the output PSD
                    chanSpct-&gt;Elem(rxPortIdx, txPortIdx, iRb) = sqrtVit * subsbandGain;
                }
            }
        }
        vit++;
        iRb++;
    }</code></pre>
<p>Yes, we did copy the sincos cache, because we can’t overwrite it with doppler data, which is updated more regularly. But we just cut the number of multiplications in half.</p>
<p>This patch is part of <a href="https://gitlab.com/nsnam/ns-3-dev/-/merge_requests/1888">ns-3 MR1888</a>.</p>
<p>The effects, again, are just as incredible as the sincos ones before it: 1.22x total simulation speedup, or 1.71x speedup in GenSpectrumChannelMatrix function.</p>
<p>Figures from before <a href="#fig:precompute-delay-doppler-before" data-reference-type="ref" data-reference="fig:precompute-delay-doppler-before">15</a> and after <a href="#fig:precompute-delay-doppler-after" data-reference-type="ref" data-reference="fig:precompute-delay-doppler-after">16</a> below show exactly the magnitude of the savings.</p>
<figure>
<img src="/assets/img/2024-03-07-the-name-is-the-speedagean/precompute-delay-doppler-before.png" id="fig:precompute-delay-doppler-before" style="width:100.0%" alt="" /><figcaption>Before optimization</figcaption>
</figure>
<figure>
<img src="/assets/img/2024-03-07-the-name-is-the-speedagean/precompute-delay-doppler-after.png" id="fig:precompute-delay-doppler-after" style="width:100.0%" alt="" /><figcaption>After optimization</figcaption>
</figure>
<p>And we still have more optimization work to do before our next nr release. When it gets released, we will make a post full of performance comparisons in the <a href="https://5g-lena.cttc.es/">5g-lena website</a>.</p>
